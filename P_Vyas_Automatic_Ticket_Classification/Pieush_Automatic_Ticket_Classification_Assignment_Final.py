# -*- coding: utf-8 -*-
"""Pieush_Navendu_Automatic_Ticket_Classification_Assignment_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wz9HtuGabAQWYLO7MMASDvLq4AqiQHME

## Problem Statement 

You need to build a model that is able to classify customer complaints based on the products/services. By doing so, you can segregate these tickets into their relevant categories and, therefore, help in the quick resolution of the issue.

You will be doing topic modelling on the <b>.json</b> data provided by the company. Since this data is not labelled, you need to apply NMF to analyse patterns and classify tickets into the following five clusters based on their products/services:

* Credit card / Prepaid card

* Bank account services

* Theft/Dispute reporting

* Mortgages/loans

* Others 


With the help of topic modelling, you will be able to map each ticket onto its respective department/category. You can then use this data to train any supervised model such as logistic regression, decision tree or random forest. Using this trained model, you can classify any new customer complaint support ticket into its relevant department.

# Prerequisit
	1. Data file on Google Drive
	2. Availability of Google colab

# Analysis & Coding Approach
## 1.	Setting up coding environment  
    1.1. Import Libraries
    1.2. Set visualization/ display parameters
    1.3. Define variables and initialize
    1.4. Define Helper Function    
## 2. Data loading
	2.1 Connect with Google Drive
	2.2 Read jason file and convert in dataframe 
	2.3 Feel the data  and other attributes attributes
	3.4 Analyse the data - Null, NULL percent etc
	2.5 Prepare dataset - Remove Null & rename column
## 3. Text preprocessing
	3.1 Make the text lowercase
	3.2 Remove text in square brackets
	3.3 Remove punctuation
	3.4 Remove words containing numbers
	3.5 Lemmatize the texts
	3.6 Use POS tags to get relevant words from the texts
## 4. Exploratory data analysis (EDA)
    4.1 Visualise No of Complaints and distribution of Complaint character length 
    4.2 Visualise top 40 words by frequency among articles using wordcloud
    4.3 Remove PRON
    4.4 Visualize top 50 Top 50 unigrams and list top 10 unigrams
    4.4 Visualize 30 bigrams and list top 10 bigrams
    4.5 Visualize 30 trigrams and list top 10 trigrams
    4.6 Remove personal data (as per business rule personal data is xxxx	
## 5.Feature Extraction
    5.1 Initialise the TfidfVectorizer
    5.2 Create the Document Term Matrix
## 6. Topic modelling 
    6.1 Manual Topic Modeling (using Gensim's NMF)
    6.2 Load NMF model with best no of topics
    6.3 print top 15 of each topics
    6.4 Create the best topic for each complaint
    6.5 Assign names to the relevant topic
"""

!pip install --upgrade gensim

"""## Pipelines that needs to be performed:

You need to perform the following eight major tasks to complete the assignment:

1.  Data loading

2. Text preprocessing

3. Exploratory data analysis (EDA)

4. Feature extraction

5. Topic modelling 

6. Model building using supervised learning

7. Model training and evaluation

8. Model inference

## 1.1.	Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import json 
import numpy as np
import pandas as pd
import re, nltk, spacy, string
import en_core_web_sm
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
# %matplotlib inline
from plotly.offline import plot
import plotly.graph_objects as go
import plotly.express as px
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from pprint import pprint
import random
from textblob import TextBlob
import nltk
from subprocess import check_output
from wordcloud import WordCloud, STOPWORDS
from sklearn.decomposition import NMF
from gensim.models import Nmf
import warnings
warnings.filterwarnings("ignore")
from gensim.corpora.dictionary import Dictionary
from gensim.models.nmf import Nmf
from gensim.models.coherencemodel import CoherenceModel
from operator import itemgetter
import pickle
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix  
from sklearn.metrics import f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB

# Download necessary packages
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')

nlp = en_core_web_sm.load()

"""## 1.2.	Set visualization/ display parameters"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_info_columns', 200)
pd.set_option('display.max_colwidth', -1)
pd.set_option('display.width', 360)
pd.set_option('float_format','{:,.4}'.format)
sns.set_style('dark')
pd.options.mode.chained_assignment = None

"""## 1.3.	Define variables and initialize"""

displarrows = 10
topXvars = 10
drawplots = True

train_size_percent = 0.75
test_size_percent = 0.25
colorlist = ['blue','orange','green','purple','brown','pink','gray','olive','cyan']
colorlistlen = len(colorlist)-1 
cmapslist = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',
            'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',
            'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']
cmapslistlen=len(cmapslist)-1

"""## 1.4.	Define Helper Functions
    Dataframe information
    Data Cleaning - Remove square brackets, remove punctuation and remove words containing numbers
    Lemmatize the texts
    Extract the POS tags
    Extract only noun, singular
    Top 30 unigram frequency  
    Top 30 bigram frequency
    Top 30 trigram frequency

### Function 1
"""

def datainfo(df):
    count = df.count()
    null = df.isnull().sum()
    null_perc = round(df.isnull().sum()/len(df.index)*100,4)
    unique = df.nunique()
    types = df.dtypes
    return pd.concat([count, null, null_perc, unique, types], 
                     axis = 1,keys=['COUNT','NULL','PERCENT','UNIQUE','DATATYPE']).sort_values(by='PERCENT',ascending=False)

"""### Remaining Functions"""

def clean_text(text):
    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    return text

#Function to Lemmatize the texts
def lemmatizer(text):        
    sent = []
    doc = nlp(text)
    for word in doc:
        sent.append(word.lemma_)
    return " ".join(sent)
    
#Write your function to extract the POS tags 
def pos_tag(text):
    try:
        return TextBlob(text).tags
    except:
        return None

def get_adjectives(text):
    blob = TextBlob(text)
    return ' '.join([ word for (word,tag) in blob.tags if tag == "NN"])

#Write your code here to find the top 30 unigram frequency among the complaints in the cleaned datafram(df_clean). 
def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

#Write your code here to find the top 30 bigram frequency among the complaints in the cleaned datafram(df_clean). 
def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

#Write your code here to find the top 30 trigram frequency among the complaints in the cleaned datafram(df_clean). 
def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]

"""# 2. Data loading"""

## **Upload the data file in Colab before executing**

"""## 2.1 Connect with Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/MyDrive/Colab Notebooks/complaints-2021-05-14_08_16.json"

"""## 2.2 Read jason file and convert in dataframe"""

# Opening JSON file 
datafile = "/content/drive/MyDrive/Colab Notebooks/complaints-2021-05-14_08_16.json"
f = open(datafile) # Write the path to your data file and load it

# returns JSON object as  a dictionary 
data = json.load(f)
df = pd.json_normalize(data)

"""## 2.3 Feel the data  and other attributes attributes"""

# Inspect the dataframe to understand the given data.
df.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

#print the column names
list(df.columns)

"""## 2.4 Analyse the data - Null, NULL percent etc"""

datainfo(df)

"""## 2.5 Prepare dataset -  rename column & Remove Null"""

# Assign new column names
df.columns = ["index","type","id","score","tags","zip_code","complaint_id","issue","date_received","state","consumer_disputed","product","company_response","company","submitted_via","date_sent_to_company","company_public_response","sub_product","timely","complaint_what_happened","sub_issue","consumer_consent_provided"]

# Assign nan in place of blanks in the complaints column
df[df['complaint_what_happened']==''] = np.nan

# Remove all rows where complaints column is nan
df=df.dropna(subset=['complaint_what_happened'])

"""# 3. Text preprocessing

## Clean Data
    3.1 Make the text lowercase
    3.2 Remove text in square brackets
    3.3 Remove punctuation
    3.4 Remove words containing numbers
"""

df.complaint_what_happened=df.complaint_what_happened.astype(str)
df_clean = pd.DataFrame(df.complaint_what_happened.apply(lambda x: clean_text(x)))

"""## 3.5 Lemmatize the texts"""

df_clean["Complaint_lemmatize"] =  df_clean.apply(lambda x: lemmatizer(x['complaint_what_happened']), axis=1)

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""## 3.6 Use POS tags to get relevant words from the texts"""

df_clean["complaint_POS_removed"] =  df_clean.apply(lambda x: get_adjectives(x['Complaint_lemmatize']), axis=1)

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""# 4. Exploratory data analysis (EDA)

## 4.1 Visualise No of Complaints and distribution of Complaint character length
"""

# To analyze distribution of Complaint Character Length
plt.figure(figsize=(10,6))
doc_lens = [len(d) for d in df_clean.complaint_POS_removed]
plt.hist(doc_lens, bins = 50)
plt.title('Distribution of Complaint character length\n',fontdict={'fontsize':15,'fontweight':5,'color':'Green'})
plt.ylabel('Number of Complaint\n',fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.xlabel('\nComplaint character length\n',fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
sns.despine()

"""## 4.2 Vsualise top 40 words by frequency among articles using wordcloud"""

# To visualize top 40 words
mpl.rcParams['figure.figsize']=(12.0,12.0)  
mpl.rcParams['font.size']=12            
mpl.rcParams['savefig.dpi']=100             
mpl.rcParams['figure.subplot.bottom']=.1 
stopwords = set(STOPWORDS)
wordcloud = WordCloud(
                          background_color='white',
                          stopwords=stopwords,
                          max_words=40,
                          max_font_size=40, 
                          random_state=42
                         ).generate(str(df_clean['complaint_POS_removed']))
print(wordcloud)
fig = plt.figure(1)
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""## 4.3  Remove PRON"""

# To remove PRON
df_clean['Complaint_clean'] = df_clean['complaint_POS_removed'].str.replace('-PRON-', '')

"""## 4.4 Visualize Top 50 unigrams and list top 10 unigrams"""

# To visualize Top 50 Unigrams
common_words = get_top_n_words(df_clean['Complaint_clean'].values.astype('U'), 50)
df2 = pd.DataFrame(common_words, columns = ['unigram' , 'count'])

fig = go.Figure([go.Bar(x=df2['unigram'], y=df2['count'])])
fig.update_layout(title=go.layout.Title(text="Top 50 unigrams in the Complaint text after removing stop words and lemmatization"))
fig.show()

# To make a list of top 10 Unigrams
df2.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""## 4.4 Visualize 30 bigrams and list top 10 bigrams"""

common_words = get_top_n_bigram(df_clean['Complaint_clean'].values.astype('U'), 30)
df3 = pd.DataFrame(common_words, columns = ['bigram' , 'count'])
fig = go.Figure([go.Bar(x=df3['bigram'], y=df3['count'])])
fig.update_layout(title=go.layout.Title(text="Top 30 bigrams in the Complaint text after removing stop words and lemmatization"))
fig.show()

df3.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""## 4.5 Visualize 30 trigrams and list top 10 trigrams"""

common_words = get_top_n_trigram(df_clean['Complaint_clean'].values.astype('U'), 30)
df4 = pd.DataFrame(common_words, columns = ['trigram' , 'count'])
fig = go.Figure([go.Bar(x=df4['trigram'], y=df4['count'])])
fig.update_layout(title=go.layout.Title(text="Top 30 trigrams in the Complaint text"))
fig.show()

df4.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""## 4.6 Remove personal data (as per business rule personal data is xxxx"""

df_clean['Complaint_clean'] = df_clean['Complaint_clean'].str.replace('xxxx','')

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""# 5.Feature Extraction

## 5.1 Initialise the TfidfVectorizer
ignore terms that appear in more than 95% of the complaints (max_df = 0.95)
ignore terms that appear in less than 2 complaints (min_df = 2)
"""

tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')

"""## 5.2 Create the Document Term Matrix"""

#Write your code here to create the Document Term Matrix by transforming the complaints column present in df_clean.
dtm = tfidf.fit_transform(df_clean['Complaint_clean'])

"""# 6.Topic Modelling

## 6.1 Manual Topic Modeling (using Gensim's NMF)
    Identify best no of topics using coherence score
"""

texts = df_clean['Complaint_clean']
dataset = [d.split() for d in texts]
dictionary = Dictionary(dataset) # Create a dictionary  (mapping between words and their integer id)
dictionary.filter_extremes(  
    no_below=3,
    no_above=0.85,
    keep_n=5000
) # Filter out extremes to limit the number of features
corpus = [dictionary.doc2bow(text) for text in dataset] # Create the bag-of-words
topic_nums = list(np.arange(5, 10, 1)) # Create list of the topic numbers to eveluate
coherence_scores = [] # calculate the coherence score for each number of topics by runing the nmf model
for num in topic_nums:
    nmf = Nmf(
        corpus=corpus,
        num_topics=num,
        id2word=dictionary,
        chunksize=2000,
        passes=5,
        kappa=.1,
        minimum_probability=0.01,
        w_max_iter=300,
        w_stop_condition=0.0001,
        h_max_iter=100,
        h_stop_condition=0.001,
        eval_every=10,
        normalize=True,
        random_state=42
    )
    cm = CoherenceModel(
        model=nmf,
        texts=texts,
        dictionary=dictionary,
        coherence='c_v'
    )     # Run the coherence model to get the score
    coherence_scores.append(round(cm.get_coherence(), 5)) 
scores = list(zip(topic_nums, coherence_scores))
best_num_topics = sorted(scores, key=itemgetter(1), reverse=True)[0][0] # highest coherence score
print(best_num_topics)

"""## 6.2 Load NMF model with best no of topics"""

#Load your nmf_model with the n_components i.e 5
num_topics = 5 #write the value you want to test out

#keep the random_state =40
nmf_model = NMF(n_components=5,random_state=40) #write your code here

nmf_model.fit(dtm)
len(tfidf.get_feature_names())

"""## 6.3 print top 15 of each topics"""

#Print the Top15 words for each of the topics
single_topic = nmf_model.components_[0]
single_topic.argsort()
top_word_indices = single_topic.argsort()[-10:]
for index in top_word_indices:
    print(tfidf.get_feature_names()[index])
for index,topic in enumerate(nmf_model.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])
    print('\n')

"""## 6.4 Create the best topic for each complaint"""

#Create the best topic for each complaint in terms of integer value 0,1,2,3 & 4
topic_results = nmf_model.transform(dtm)
topic_results[0].round(2)
topic_results[0].argmax()
topic_results.argmax(axis=1)

#Assign the best topic to each of the cmplaints in Topic Column
df_clean['Topic'] = topic_results.argmax(axis=1)  #write your code to assign topics to each rows.

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

#Print the first 5 Complaint for each of the Topics
df_clean=df_clean.groupby('Topic').head(5)
df_clean.sort_values('Topic')

"""## 6.5 Assign names to the relevant topic
    0 Bank Account services
    1 Credit card or prepaid card
    2 Theft/Dispute Reporting
    3 Mortgage/Loan
    4 Others
"""

#dictionary of Topic names and Topics
Topic_names = {0:"Bank Account services",1:"Credit card or prepaid card", 2:"Others",3:"Theft/Dispute Reporting",4:"Mortgage/Loan"}
df_clean['Topic'] = df_clean['Topic'].map(Topic_names) #Replace Topics with Topic Names

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""# 7. Model building using supervised learning

## 7.1 Create Dictionary and replace topic with name
"""

#Create the dictionary again of Topic names and Topics
Topic_names = {"Bank Account services":0,"Credit card or prepaid card":1,"Others":2,"Theft/Dispute Reporting":3,"Mortgage/Loan":4}

#Replace Topics with Topic Names
df_clean['Topic'] = df_clean['Topic'].map(Topic_names)

df_clean.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

#Keep the columns"complaint_what_happened" & "Topic" only in the new dataframe --> training_data
training_data=df_clean[["complaint_what_happened","Topic"]]

training_data.head(displarrows).style.background_gradient(cmap=cmapslist[random.randrange(0, cmapslistlen)])

"""## 7.2 Get vector count & transform word vector to tf-idf"""

#Write your code to get the Vector count
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(training_data.complaint_what_happened)
pickle.dump(count_vect.vocabulary_, open("count_vector.pkl","wb"))

#Write your code here to transform the word vector to tf-idf
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
pickle.dump(tfidf_transformer, open("tfidf.pkl","wb"))

"""## 8.1 Build Logistic regression"""

# Logistic Regression
X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=test_size_percent, random_state=42)
clf = LogisticRegression(random_state=0).fit(X_train, y_train)
pickle.dump(clf, open("logreg_model.pkl", "wb")) #SAVE MODEL

target_names = ["Bank Account services","Credit card or prepaid card","Others","Theft/Dispute Reporting","Mortgage/Loan"]
docs_new = "I can not get from chase who services my mortgage, who owns it and who has original loan docs"
docs_new = [docs_new]
loaded_vec = CountVectorizer(vocabulary=pickle.load(open("count_vector.pkl", "rb"))) #LOAD MODEL
loaded_tfidf = pickle.load(open("tfidf.pkl","rb"))
loaded_model = pickle.load(open("logreg_model.pkl","rb"))
X_new_counts = loaded_vec.transform(docs_new)
X_new_tfidf = loaded_tfidf.transform(X_new_counts)
predicted = loaded_model.predict(X_new_tfidf)
print(target_names[predicted[0]])

predicted = loaded_model.predict(X_test)
result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})
for predicted_item, result in zip(predicted, y_test):
    print(target_names[predicted_item], ' - ', target_names[result])

"""### 8.2 Print Confusion Matrix and classification_report (Logistic regression)"""

confusion_mat = confusion_matrix(y_test,predicted)
print(confusion_mat)
target_names = ["Bank Account services","Credit card or prepaid card","Mortgage/Loan","Theft Reporting","Others"]
classification_report_Logisticregression = classification_report(y_test, predicted, target_names=target_names)
print(classification_report_Logisticregression)

"""## 8.3 Build Decision Tree"""

X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=test_size_percent, random_state=42)
clf = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)
pickle.dump(clf, open("DT_model.pkl", "wb")) #SAVE MODEL

target_names = ["Bank Account services","Credit card or prepaid card","Others","Theft/Dispute Reporting","Mortgage/Loan"]
docs_new = "I can not get from chase who services my mortgage, who owns it and who has original loan docs"
docs_new = [docs_new]
loaded_vec = CountVectorizer(vocabulary=pickle.load(open("count_vector.pkl", "rb"))) #LOAD MODEL
loaded_tfidf = pickle.load(open("tfidf.pkl","rb"))
loaded_model = pickle.load(open("DT_model.pkl","rb"))
X_new_counts = loaded_vec.transform(docs_new)
X_new_tfidf = loaded_tfidf.transform(X_new_counts)
predicted = loaded_model.predict(X_new_tfidf)
print(target_names[predicted[0]])

predicted = loaded_model.predict(X_test)
result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})
for predicted_item, result in zip(predicted, y_test):
    print(target_names[predicted_item], ' - ', target_names[result])

"""## 8.4 Print Confusion Matrix and classification_report (Decision Tree)"""

confusion_mat = confusion_matrix(y_test,predicted)
print(confusion_mat)
from sklearn.metrics import classification_report
target_names = ["Bank Account services","Credit card or prepaid card","Mortgage/Loan","Theft Reporting","Others"]
classification_report_DecisionTree = classification_report(y_test, predicted, target_names=target_names)
print(classification_report_DecisionTree)

"""## 8.5 Build Random Forest"""

# Random Forest
X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=test_size_percent, random_state=42)
clf = RandomForestClassifier(max_depth=10, random_state=0)
clf.fit(X_train, y_train)
#SAVE MODEL
pickle.dump(clf, open("RF_model.pkl", "wb"))

target_names = ["Bank Account services","Credit card or prepaid card","Others","Theft/Dispute Reporting","Mortgage/Loan"]
docs_new = "I can not get from chase who services my mortgage, who owns it and who has original loan docs"
docs_new = [docs_new]

#LOAD MODEL
loaded_vec = CountVectorizer(vocabulary=pickle.load(open("count_vector.pkl", "rb")))
loaded_tfidf = pickle.load(open("tfidf.pkl","rb"))
loaded_model = pickle.load(open("RF_model.pkl","rb"))

X_new_counts = loaded_vec.transform(docs_new)
X_new_tfidf = loaded_tfidf.transform(X_new_counts)
predicted = loaded_model.predict(X_new_tfidf)

print(target_names[predicted[0]])

predicted = loaded_model.predict(X_test)
result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})
for predicted_item, result in zip(predicted, y_test):
    print(target_names[predicted_item], ' - ', target_names[result])

"""## 8.6 Print Confusion Matrix and classification_report (Random Forest)"""

confusion_mat = confusion_matrix(y_test,predicted)
print(confusion_mat)
target_names = ["Bank Account services","Credit card or prepaid card","Mortgage/Loan","Theft Reporting","Others"]
classification_report_RandomForest = classification_report(y_test, predicted, target_names=target_names)
print(classification_report_RandomForest)

"""## 8.7 Multinomial Naive Bayes"""

X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, training_data.Topic, test_size=test_size_percent, random_state=42)
clf = MultinomialNB().fit(X_train, y_train) # Multinomial Naive Bayes
pickle.dump(clf, open("nb_model.pkl", "wb")) #SAVE MODEL

target_names = ["Bank Account services","Credit card or prepaid card","Others","Theft/Dispute Reporting","Mortgage/Loan"]
docs_new = "I can not get from chase who services my mortgage, who owns it and who has original loan docs"
docs_new = [docs_new]
loaded_vec = CountVectorizer(vocabulary=pickle.load(open("count_vector.pkl", "rb"))) #LOAD MODEL
loaded_tfidf = pickle.load(open("tfidf.pkl","rb"))
loaded_model = pickle.load(open("nb_model.pkl","rb"))
X_new_counts = loaded_vec.transform(docs_new)
X_new_tfidf = loaded_tfidf.transform(X_new_counts)
predicted = loaded_model.predict(X_new_tfidf)
print(target_names[predicted[0]])

predicted = loaded_model.predict(X_test)
result_bayes = pd.DataFrame( {'true_labels': y_test,'predicted_labels': predicted})
for predicted_item, result in zip(predicted, y_test):
    print(target_names[predicted_item], ' - ', target_names[result])

"""## 8.8 Print Confusion Matrix and classification_report (Multinomial Naive Bayes)"""

confusion_mat = confusion_matrix(y_test,predicted)
print(confusion_mat)
target_names = ["Bank Account services","Credit card or prepaid card","Mortgage/Loan","Theft Reporting","Others"]
classification_report_NaiveBayes = classification_report(y_test, predicted, target_names=target_names)
print(classification_report_NaiveBayes)

"""## 9. Model inference"""

print("****classification_report_Logisticregression_____________________________________________")
print(classification_report_Logisticregression)
print("****classification_report_DecisionTree_____________________________________________")
print(classification_report_DecisionTree)
print("****classification_report_RandomForest_____________________________________________")
print(classification_report_RandomForest)
print("****classification_report_NaiveBayes_____________________________________________")
print(classification_report_NaiveBayes)

"""# 9.2 Inferences:"""

# Plot comparisions
import pandas as pd
import matplotlib.pyplot as plt
data = [['Logistic Regression',29,50,100,67],
		['Decision Tree',43,20,100,33],
		['Random Forest',29,50,100,67],
		['Naive Bayes',29, 100, 100, 100]
       ]  
df_inferences = pd.DataFrame(data,columns=['Model','Accuracy', 'Precision', 'Recall', 'F1-Score'])  # Create DataFrame
df_inferences.head(10)

plt.figure(figsize = (20,6))
plt.subplot(1, 4, 1)
plt.bar(df_inferences['Model'], df_inferences['Accuracy'],  width=.5,color =colorlist[random.randrange(0, colorlistlen-1)])
plt.xticks(rotation=90)
plt.title('Accuracy')
plt.subplot(1, 4, 2)
plt.bar(df_inferences['Model'], df_inferences['Precision'],  width=.5,color =colorlist[random.randrange(0, colorlistlen-1)])
plt.xticks(rotation=90)
plt.title('Precision')
plt.subplot(1, 4, 3)
plt.bar(df_inferences['Model'], df_inferences['Recall'],  width=.5,color =colorlist[random.randrange(0, colorlistlen-1)])
plt.xticks(rotation=90)
plt.title('Recall')
plt.subplot(1, 4, 4)
plt.bar(df_inferences['Model'], df_inferences['F1-Score'],  width=.5,color =colorlist[random.randrange(0, colorlistlen-1)])
plt.xticks(rotation=90)
plt.title('F1-Score')
plt.show()

"""# Inferences:
  - As per the comparision of different modeling methods Accuracy for different model is as folloinwgs...
  a) **Logistic Regression:** Overall Accuracy: **29%**, whereas Precision, Recall and F1-Score for Credit Card or Prepaid Card is **50%**, **100**, and **67%** respectively.

  b) **Decision Tree**: Overall Accuracy: **43%**, whereas Precision, Recall and F1-Score for Credit Card or Prepaid Card is **20%**, **100**, and **33%** respectively.

  c) **Random Forest**: Overall Accuracy: **29%**, whereas Precision, Recall and F1-Score for Credit Card or Prepaid Card is **50%**, **100**, and **67%** respectively.

  d) **NaiveBayes**: Overall Accuracy: **29%**, whereas Precision, Recall and F1-Score for Credit Card or Prepaid Card is **100%**.

# Conclusion
Manual process of ticket classification in any ITSM Tools like ServiceNow or BMC Remedy can be effort-intensive and thus costly. A manual process will require 3 shifts to cover 24x7 operation.
 
With the developments in data science allowing for automated natural language processing and ability to make classification based on algorithms, it is possible to disrupt the manual process and bring dramatic improvement in efficiencies.
 
A solution in combination with the ITSM Tool to automate the process of classifying tickets to appropriate type and hence assigning to resolver groups could give a huge gain in operational effeciency and cost saving.

The above algorithm including supervised and insupervised model is a practical approach that can be used by any application maintenance project using any ITSM (ticketing tool). The input for this algorithm is text description with certain information. Supervised as well as unsupervised text analytics is applied to provide data insights in form of 
ticket classification.

The benifits can be stated as
*   Round-the-clock Operation
*   Effort Optimization
*   Improved Incident Resolution Time
*   Opportunity to initiate automated corrective actions using Robotics Automation Process or triggering automates scripts for automatic resolution
"""

