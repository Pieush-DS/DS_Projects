# -*- coding: utf-8 -*-
"""UOA_P_Vyas_DataEthics_churn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nEWKgwjuA7kdtoHQO1rZvMxy91BDboW

# <font color = blue> Data Ethics  </font>

### Problem Statement:

- Need to understand the buying pattern and trends of the existing customers based on the data provided to you. Second, you need to understand which customers are likely to churn out and why?
Remember, both the tasks, identifying patterns and trends and building a customer churn model, could be based on assumptions purely out of your understanding of the problem statement.

- Introduction:

  As an online grocery delivery company, “Everyday Deals,” you are part of the data science team to scale up the business with the power of data. As part of the organization’s attempts to increase its sales, revenue, and profit and ultimately cover more markets in the country, your team is tasked to generate better product recommendations to existing customers so that there is higher customer engagement and reduced churn rate.

## About the datset
Based on the dataset provided, here is a brief description of each column:

**Order**: A unique identifier for each order.

**Member**: A unique identifier for each member/customer.

**SKU**: A list of product identifiers associated with each order.

**Created On**: The date when the order was created.

**Description**: Description of the products included in the order.

**Member's Full Name**: Full name of the member/customer.

**Member's Address**: Address of the member/customer.

**Member's Email**: Email address of the member/customer.

**Member's Phone Number**: Phone number of the member/customer.

**Member's Gender**: Gender of the member/customer (Male or Female).

**Member's Date of Birth**: Date of birth of the member/customer.

**Member's Membership Level**: Membership level of the member/customer (e.g., Gold, Silver, Bronze).

**Member's Purchase History**: Number of previous purchases made by the member/customer.

**Order Value**: Total value of the order.

**Payment Method**: Payment method used for the order (e.g., PayPal, Credit Card, Cash).

**Delivery Address**: Address to which the order is delivered.

**Order Status**: Status of the order (e.g., Shipped, Delivered, Pending).

**Credit Card Number**: Credit card number used for payment (masked in the provided dataset).

## Provided dataset is:

Unstructured Data: The dataset appears to have some unstructured data, such as multiple product identifiers in the SKU column, and the description column containing free-text descriptions of the products. This can make data processing and analysis more challenging.

Missing Values: The dataset seems to have missing values in some columns, such as Member's Purchase History and Credit Card Number. These missing values can impact the accuracy and reliability of the analysis.

Bias: The dataset might be biased towards certain types of members or products, which can affect the generalizability of the results and insights obtained from the data.

Data Quality: There could be inconsistencies or errors in the data, such as incorrect phone numbers, email addresses, or dates of birth. Ensuring data quality is crucial for obtaining accurate results.

Data Privacy: The dataset contains sensitive information such as email addresses, phone numbers, and credit card numbers. Proper data privacy measures should be taken to protect this information.

Data Transformation: Some data columns, like date of birth, may need to be transformed or normalized to a consistent format for analysis.

Feature Engineering: Additional features or variables might need to be derived from the existing data to improve the predictive power of the models.

Data Visualization: Effective data visualization techniques can be employed to better understand the data and identify patterns and trends.

Data Preprocessing: Cleaning, transforming, and preparing the data are crucial steps to ensure accurate and meaningful analysis.

Overall, addressing these points will be essential for conducting a reliable and insightful analysis on this dataset.

### Steps to Follow...
1. Importing Libraries

2. Data Preprocessing

3. Data Cleaning

4. Exploratory Data Analysis

5. Feature selection

6. Training the Model

7. Model Building
    - A) Logistic Regression
    - B) Random Forest
    - C) Decision Tree
    - D) Hyperparameter tuning using RandomizedSearch CV
    - E) XG Boost
    - F) VotingClassifier
      
8. Conclusion

## 1. Importing libraries
"""

# For data manipulation and analysis
import pandas as pd

# For numerical and mathematical operations on arrays
import numpy as np


# Data visualization library
import matplotlib.pyplot as plt
import seaborn as sns


# Provides regular expression support
import re

# Split data into training and testing sets for machine learning model evaluation.
from sklearn.model_selection import train_test_split

# Importing libraries for building models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier

# Standardizes features
from sklearn.preprocessing import StandardScaler

# Evaluation matrix
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score,f1_score,accuracy_score
from sklearn.model_selection import RandomizedSearchCV


# Remove warnings
import warnings
warnings.filterwarnings('ignore')

# To set the Row, Column and Width of the Dataframe to show on Jupyter Notebook
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

"""## 2. Data Preprocessing

- We started by loading the dataset and checking for missing values.

- Then, we performed data cleaning to remove any NaN values and duplicated rows.


"""

# Loading the datset
churn = pd.read_excel('Data+ethics+assignment+-+dataset.xlsx')

# Returning the first 5 rows
churn.head()

# Checking the shape of data
churn.shape

# Information of data
churn.info(verbose=True)

"""# 3) Data Cleaning"""

# To check any duplicate value in various columns
churn.duplicated().sum()

"""- There are no duplicate values for all the rows."""

# Calulating null values
null_values = churn.isnull().sum()
null_values

# Calculating the percentage of null values for each feature
percentage_null_values = round(100*(churn.isnull().sum()/len(churn.index)), 2)
percentage_null_values

"""- As per the above null value analysis variable **Credit Card Number has nearly 67.06%** null values.Which is very high value of null. Therefore, we need to drop this column."""

# To drop Credit Card Number column
churn.drop(['Credit Card Number'], axis=1, inplace=True)

# Calculating the percentage of null values for each feature
percentage_null_values = round(100*(churn.isnull().sum()/len(churn.index)), 2)
percentage_null_values

"""- Now there is no null values in any variable."""

# Changing the name of columns for better outputs
column_mapping = {column: column.replace("Member's ","") for column in churn.columns if "Member's " in column}
churn = churn.rename(columns=column_mapping)
churn.head()

# Information of data
churn.info(verbose=True)

"""- As per the above analysis **14 columns having object data type** and **3 columns are of int** data type.

"""

# To see the statistical aspects of the dataframe
round(churn.describe(percentiles = [0, 0.25, 0.5, 0.75, 0.80, 0.85, 0.90, 0.95, 0.99]), 2)

"""- As per the above statistical description there is no such outliers in the dataframe.

# 4) Exploratory Data Analysis
"""

# To recheck the correlation coefficients to know which variables are highly correlated
churn_cor = churn.corr()
churn_cor

# To plot the Heatmap for all the varaibles
fig = plt.figure(figsize = (6, 5))
sns.heatmap(churn_cor, cmap="RdYlGn", annot = True )
plt.title("Correlation between various numercial varibales\n", fontdict={'fontsize':12,'fontweight':5,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':12,'fontweight':5,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':12,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- As per the above heatmap it is clear that there is no such coorelations among the various numerical variables.
"""

# Checking the unique numbers of Order in our dataset
churn['Order'].nunique()

"""- There are total **8387** unique Orders"""

#checking the unique numbers of Member in our dataset
churn['Member'].nunique()

"""- There are total **106** unique Members."""

# Checking the total count of each Member in our dataset
churn['Member'].value_counts()

# viewing all the columns names
churn.columns

# Droping the columns whixh are not proving much useful information
selected_columns = ['SKU', 'Full Name', 'Address', 'Email', 'Delivery Address', 'Description' ,'Purchase History','Order Status' , 'Payment Method','Membership Level' ]
churn.drop(selected_columns, axis=1, inplace=True)

# To check all the variables label
churn.head()

"""- We have droped **SKU, Full Name, Address, Email, Delivery Address, Description, Purchase History, Order Status, Payment Method, Membership Level** because few of these columns contains sensitive
information and moreover they are not providing any useful information in deciding weather a customer will churn or not.

"""

# Grouping the data on the basis of created on and order value
grouped_data = churn.groupby(['Created On', 'Order Value']).size().reset_index(name='Count')

print(grouped_data)

# Displaying  the top 20 rows
churn.head(20)

"""### Inference

The output 'grouped_data' will be a new DataFrame containing the unique combinations of 'Created On' and 'Order Value',
along with the count of occurrences for each combination. It shows how many times each specific combination appeared in the original 'df' DataFrame.
"""

# Filter records with missing phone numbers
filtered_df = churn[churn['Phone Number'].isna()]
print(filtered_df.shape[0])

# Pattern to find numbers starting with a hyphen '-'
pattern = r'0\d+'

# Checking if we have phone numbers where length of phone number is less than 10
matching_phone_numbers =churn[churn['Phone Number'].str.len() < 10]

# To count variable
matching_phone_numbers.count()

# Use str.contains with regex=True to find matching phone numbers
# To fetch records with a particular pattern present for eg '-'
matching_phone_numbers = churn[churn['Phone Number'].str.contains(pattern, regex=True, na=False)]
print(matching_phone_numbers)

# To get Member wise total phone numbber count
phone_data = churn.groupby('Member')['Phone Number'].count().reset_index(name='Total Phone Numbers')
print(phone_data)

"""### Inference:

- The output 'phone_data' will be a new DataFrame containing the unique combinations of 'Member' and 'Phone Number',
along with the count of occurrences for each combination. It shows how many times each specific combination appeared
in the original 'df' DataFrame. It is not providing any useful information since each member hold multiple phone number.
Therfore we can conclude that we can drop this column.

### What we should do for getting better visibily of the records in our data??

These steps might help us to get better visibily


1) we can predict the order value for the given customers.

2) we can predit which customer will churn and which customers will not churn

3) Check if  age plays any important role..?
"""

# Grouping the data on the basis of Member and Gender
gender_df = churn.groupby(['Member' , 'Gender'])

for group_name, group_data in gender_df:
    print(f"Member, Gender: {group_name}")
    print(group_data.head(3))
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)

"""### Inferences:
- Here we can notice that for each member id both Male and Female gender has been provided which is logically not possible.
So we can consider this as a system error which might have occured during entery data. Now we should focus on the total
count of each member.
"""

# Visualizing the box plot for the numerical datatype columns which help us to find the outliers, if present in our dataset.

for col_name in churn.columns:
    if churn[col_name].dtypes == 'int64':
        plt.figure(figsize=(2,2)) # Ajust the figure size
        sns.boxplot(x=churn[col_name], palette="Set1")  # Add the 'palette' parameter here
        plt.xlabel(col_name)
        plt.ylabel('count')
        plt.show()

"""### Inferences:
- In the above box plot we can see there are outliers in our order columns. But we can not drop these valuessinces order value is an important factor in making our decision and if we remove these values we might end up in losing some important information.

"""

# For getting the tentative age of each member we need to convert 'Date of Birth' column to datetime objects.
churn['Date of Birth'] = pd.to_datetime(churn['Date of Birth'], errors='coerce')

# Calculate the age by subtracting 'Date of Birth' from the current date
from datetime import datetime
current_date = datetime.now()
churn['Age'] = (current_date - churn['Date of Birth']).astype('<m8[Y]')

# Grouping the records on the basis of sum of total order value, calculated age and the total count of orders placed by each member.
grouped_df =churn.groupby('Member').agg({
    'Order Value': 'sum',   # Total sum of 'Order Value' for each member
    'Age': 'first',         # Assuming 'Age' is the same for each member (you can use 'first' to get the age of the first occurrence)
    'Order': 'count'        # Total number of orders for each member
}).reset_index()
print(grouped_df)

"""### Inferences:
    
- On grouping the records on the basis of sum of total order value, calculated age and the total count of orders placed by
each member we can find the members which are placing more and higher values orders. This information can be very useful
in decsion making.
"""

# Since we are having a huge data so get a better inference we can do binning. This will divide our age in various groups.
age_bins = [10, 30, 60, 80, 100, 110, float('inf')]  # You can modify the age ranges here
age_labels = ['Young', 'Teenagers' , 'Adult', 'Sn_Senior' , 'Super_Senior' , 'Over_age']

# Create a new column 'Age Group' to store the binned age categories
churn['Age Group'] = pd.cut(churn['Age'], bins=age_bins, labels=age_labels, right=False)

# Display the top 5 rows  with the new 'Age Group' column
churn.head()

# Droping the date of birth column since this column is not required anymore.
churn.drop(['Date of Birth'] , axis = 1, inplace=True)

# Display the top 5 rows
churn.head()

# Count of each type of age group present in our dataset.
churn['Age Group'].value_counts()

"""- As per the above analysis maximum members are of Teenagers. Whereas Se_Senior, Adult, and Young are top 2nd, 3rd, and 4th members types respectively.

Making a **copy of cleaned dataset** for further processing. By making a copy of the DataFrame, you ensure data
integrity and avoid potential issues that might arise from modifying the original data directly.It is important
for several more reasons.Few of them are:
    
1) Data Preservation

2) Independent Manipulation

3) Iterative Analysis

4) Avoiding Unintended Side Effects
"""

# Making a copy of dataframe
df_copy = churn.copy(deep=True)

"""### Encoding our numerical data

Machine learning algorithms work with numerical data, and **ENCODING** helps in representing categorical variables as numbers, making it easier for models to understand and process the data. It allows us to leverage the information from categorical features and enables the algorithms to learn patterns and make predictions effectively.
"""

# Encoding Gender and Age group column.
encod_col = ['Gender' , 'Age Group']
encod_dict = {}
for i in encod_col:
    ulist = list(df_copy[i].unique())
    incod_dict = {}
    for n, u in enumerate(ulist):
        incod_dict[u] = n
    encod_dict[i] = incod_dict

encod_dict

"""- In the code, the 'encod_dict' is a dictionary that maps each categorical variable to its corresponding numerical encoding. The loop iterates through each column in the DataFrame 'df_copy' and replaces the categorical values in that column with their numerical equivalents based on the mappings in the 'encod_dict'.
- According to the above categories are defined as below...
    - 0:- Young
    - 1:- Over_age
    - 2:- Sn_Senior
    - 3:- Adult
    - 4:- Super_Senior
    - 5:- Nan
    - 6:- Teenagers
"""

# Iterates through each column in the DataFrame 'df_copy' and see
for i in encod_dict:
    df_copy[i] = df_copy[i].map(encod_dict[i])

# To see the dataframe variables/features
df_copy.head()

# Checking for the null values in the Age Group columns
df_copy.isna().sum()

# To remove na values
df_copy = df_copy.dropna(subset=['Age Group'])

# To recheck null values in the Age Group columns
df_copy.isna().sum()

# To check variables
df_copy.head()

# Information of data
df_copy.info(verbose=True)

# Creating a bar chart showing the number of members in each age group
age_group_counts = df_copy['Age Group'].value_counts().sort_values(ascending=False)

# Sort age groups based on their counts
sorted_age_groups = age_group_counts.index

# Create a DataFrame with age group and their counts
age_group_df = pd.DataFrame({'Age Group': sorted_age_groups, 'Count': age_group_counts})

# Create the bar plot using the sorted order
plt.figure(figsize=(6, 5))
sns.barplot(x='Age Group', y='Count', data=age_group_df, order=sorted_age_groups)
plt.title("Number of Members in Each Age Group\n", fontdict={'fontsize':15,'fontweight':5,'color':'Green'})
plt.xlabel("\nAge Group\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Count\n", fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- Above Bar chart shows the distribution of members across different age groups. The height of each bar represents the number of members in each age group.
- As per the bar graph it is clear that members related to age group '6' (Teenagers) are highest, whereas Age group '1' (Over_age) are lowest number of members, however group '2' (Sn_Senior), group '3' (Adult), and group '0' (Young) are top 2nd, 3rd, and 4th groups of members.

- Whereas groups are as followings...
    - 0:- Young (<10 Yrs.)
    - 1:- Over_age (>100 Yrs.)
    - 2:- Sn_Senior (60-80 Yrs.)
    - 3:- Adult (30-60 Yrs.)
    - 4:- Super_Senior (80-100 Yrs.)
    - 6:- Teenagers (10-30 Yrs.)
"""

# The variable rfm_m represents the total order value for each member, obtained by grouping the data by 'Member' and summing the 'Order Value' column.
# This information allows us to understand the overall purchase behavior of each member, identifying high-value customers based on their total order value.
rfm_m = df_copy.groupby('Member')['Order Value'].sum()
rfm_m.head()

# Calculate the number of orders made by each member, counting the occurrences of each member in the 'Member' column.
total_ordervalue = df_copy.groupby('Member')['Order Value'].size()
total_ordervalue.head()

# Changing the datatype of 'Age Group' column.
df_copy['Age Group'] = df_copy['Age Group'].astype('int64')

# Information of data
df_copy.info(verbose=True)

"""- Now **droping  the records where age is less than 20 years and greater than 90 years**. Age values below 20 or above 90 may be considered extreme and could lead to skewed analysis or biased model training. By excluding these extreme values, the analysis and model predictions become more accurate and reliable for the majority of the target population within a reasonable age range."""

# Dropping the records where age is less than 20 years and greater than 90 years.
df_copy =  df_copy[(df_copy['Age'] >= 20) & (df_copy['Age'] <= 90)]

# Returning the first 5 rows
df_copy.head()

# Checking the shape of our new dataframe
df_copy.shape

# To make a copy of dataframe
churn1 = df_copy.copy(deep=True)

# To drop columns
churn1.drop(['Phone Number' , 'Age'] , inplace = True , axis =1)

# To chheck variables of dataframe
churn1.head()

# To get total order value by members
total_order_value_per_member = churn1.groupby('Member')['Order Value'].sum()
total_order_value_per_member

"""### Inferences:
- To group each member with respect to 'created on' based on every month, we need to extract the month from the 'created on' column and then group by both 'Member' and the extracted month.
"""

# Information of data
churn1.info(verbose=True)

# To convert 'Created on' column to datetime format
churn1['Created On'] = pd.to_datetime(churn1['Created On'])

# Information of data
churn1.info(verbose=True)

# Extract month from the 'Created on' column
churn1['Month'] = churn1['Created On'].dt.month

# Group by 'Member' and 'Month' and calculate the sum of 'Order Value' for each group
grouped_data = churn1.groupby(['Member', 'Month'])['Order Value'].sum().reset_index()

# To see 'grouped_data'
grouped_data

"""- Above details is showing members monthly order pattern"""

# Calculating  month_order_value
month_order_value = grouped_data.groupby('Month')['Order Value'].sum().sort_values(ascending=False)

# Sort months based on total order value in descending order
sorted_months = month_order_value.index

# Create a DataFrame with months and their total order values
month_order_df = pd.DataFrame({'Month': sorted_months, 'Total Order Value': month_order_value})

# Create the bar plot using the sorted order
plt.figure(figsize=(6, 6))
sns.barplot(x = 'Month', y = 'Total Order Value', data = month_order_df, order = sorted_months)
plt.title('Monthise Order value\n', fontdict={'fontsize':15,'fontweight':5,'color':'Green'})
plt.xlabel('Month\n',fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel('Total Order Value\n',fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- As per the above bar grph it is clear that in 8th month orders are maximum (>25000), wheras in the 11th month total order value is lowest (between 15000-20000).
- For the 9th and 6th month difference in total order value is not much but still orders in 9th month are bit higher than 6th month and these two months are top 2nd and 3rd highest of the graph.

### Pie Chart

- To create a pie chart showing the order value for a whole year for a single member, we need to first filter the data for that specific member and then calculate the total order value for each month. Once we have the total order value for each month,we can create a pie chart using the matplotlib library
"""

# lets see it for any member
member_id = 'M04158'

# Filter the data for the specific member
filtered_data = churn1[(churn1['Member'] == member_id) & (churn1
                                                          ['Created On'].dt.year == 2014)]

# Calculate the total order value for each month
grouped_data = filtered_data.groupby(filtered_data['Created On'].dt.month)['Order Value'].sum()

# Pie chart
plt.figure(figsize=(4, 4))
plt.pie(grouped_data, labels=grouped_data.index, autopct='%1.1f%%', startangle=90)
plt.title('Order Value for Member {} in 2014'.format(member_id),fontdict={'fontsize':12,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- Total order for a Member (M04158) in year 2014 the **most significant category is 4** which has highest % of orders (**34.2%**) whereas **least significant category is 7** in which % of orders are only **1.2%**. Likewise the above Pie chart is showing the order value pattern of a particular Member for a certain Year.

### To get Retained/Not-Churn (0) and Not-retained/Churn (1) members.
- Lets compare the total sum of order value of each member for each year and predict which member will retain and which member will not retain, we can follow these steps:

- Calculate the total sum of order value for each member for each year ( 2013 and 2014).

- Identify the members who have made orders in all the years (retained members).

- Identify the members who have made orders in some years but not all (non-retained members).

- Add a column 'Churn' to the dataset to indicate whether the member is retained (0) or not retained (1).
"""

# Filter the data for the years 2013, and 2014
years = [2013, 2014]
filtered_data = churn1[churn1['Created On'].dt.year.isin(years)]

# Convert the 'Order Value' column to numeric type
filtered_data['Order Value'] = pd.to_numeric(filtered_data['Order Value'])

# Group the filtered data by 'Member' and 'Created On' year and calculate the total sum of 'Order Value'
grouped_data = filtered_data.groupby(['Member', filtered_data['Created On'].dt.year])['Order Value'].sum()

# Unstack the grouped data to create a pivot table with members as rows and years as columns
pivot_table = grouped_data.unstack(fill_value=0)

# Add columns for missing years and set their values to 0
for year in years:
    if year not in pivot_table.columns:
        pivot_table[year] = 0

# Sort columns in ascending order
pivot_table.sort_index(axis=1, inplace=True)

# Create a new column 'Churn' and set it to 0 for not churned  members and 1 for churned members
churn1['Churn'] = 0

# Iterate over each member and check if the total sales value is decreasing (indicating churn)
for member in pivot_table.index:
    sales_values = pivot_table.loc[member].values
    is_increasing = all(sales_values[i] <= sales_values[i+1] for i in range(len(sales_values)-1))
    if not is_increasing and all(sales_values):
        churn1.loc[churn1['Member'] == member, 'Churn'] = 1

# Count the number of retained and churned members
churn_count = churn1['Churn'].value_counts()
churn_count

"""### Inferences:
- As per the above count it is clear that the **Churned (1) members (3370)** are more than **Not-Churned (0) members (1782)**.
"""

# Frequency distribution graph or  Histogram for year 2013 & 2014
plt.figure(figsize=(8, 5))
plt.hist(pivot_table.values, bins=20, label=['2013', '2014'])
plt.title("Total Order Value counts for 2013 vs 2014\n",fontdict={'fontsize':15,'fontweight':5,'color':'Green'})
plt.xlabel("\nTotal Order Value\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Frequency\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.legend()
plt.tight_layout()
plt.show()

"""### Inferences:
- Highest frequency/count for the Total orders is between 7500 and 15000 for the year 2014.
- Whereas between 15000 to 25000 total order alue counts is higher for the year 2013.
- Although at most of the places Total Order value frequency count is higher for year 2014 but at few places it is higher for year 2013.
"""

# To get top 5 rows of pivot table for the year 2013 and 2014
print(pivot_table.head())

# To see the variables fo dataframe
churn1.head(10)

# To get the information of the dataframe
churn1.info(verbose=True)

# Replacing M with zero for each member.
# This step is likely required to convert the 'Member' values from a format like 'M04158' to a numeric format like '04158'.
churn1['Member'] = churn1['Member'].str.replace('M', '0')

# Use the pd.to_numeric() function to convert the specified columns to numeric data type
columns_to_convert = ['Order', 'Member']
churn1[columns_to_convert] = churn1[columns_to_convert].apply(pd.to_numeric, errors='coerce')

# Print the top 5 rows of dataframe
churn1.head()

# Checking the datatype
churn1.dtypes

# Making a copy of churn dataframe.
df2 = churn1.copy(deep=True)

# Visualizing distribution of numerical variables in the DataFrame and quick overview of the data's central tendency, Spread and potential outliers.
df2.hist(figsize=(10,10))

"""### Inferences:

- Most of the features seem to be right-skewed, which means they have a higher concentration of lower values and some extreme higher values.

- The 'Order Value' feature has a long tail of high values, indicating that there are some orders with significantly higher values compared to the rest.

- The 'Age' feature shows a concentration of values around certain age groups, indicating that there may be specific age segments or groups of customers.

"""

# To plot the Heatmap for all the varaibles
fig = plt.figure(figsize = (6, 5))
sns.heatmap(df2.corr(), cmap="YlGnBu", linewidths = 2, fmt='.2f', annot = True )
plt.title("Correlation between various numercial varibales\n", fontdict={'fontsize':12,'fontweight':5,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':12,'fontweight':5,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':12,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- As per the above correlation graph it is clear that there is no much correlation among all the variables.

### Pair Plot
- The pair plot allows us to visualize the relationships between different numerical features and how they vary with the 'Churn' status. It provides insights into how different pairs of features are related and if there are any noticeable patterns or differences between churned and retained customers.
"""

# To plot the pair plot
sns.pairplot(df2,hue = 'Churn')

"""### Inferences:
- As per the above pair plot it is clear that there is no much correlation among all the variables.
"""

# Display column names
df2.columns

"""#  5) Feature Selection"""

# Select a subset of columns from the DataFrame df2 &assigns them to variable X
X = df2[['Order', 'Member', 'Gender', 'Order Value', 'Age Group', 'Month']]

# one-hot encoding on the categorical columns of X
X = pd.get_dummies(X)

# Displays the first few rows of the preprocessed data in X
X.head()

# Assigning churn column as target column
y = df2.Churn

"""- The code prepares the dataset X with feature columns, and the target variable y containing the 'Churn' labels. The one-hot encoding ensures that categorical variables are represented in a format suitable for the machine learning algorithm. The resulting dataset X is now ready for use in training a machine learning model to predict customer churn based on the selected features.

# 6) Training the model

We will start by training a classification model to predict the likelihood of Member to Churn. In addition to splitting the data into training and test sets of features and labels, we'll extract sensitive features that are used to define subpopulations of the data for which you want to compare fairness.

### Splits the preprocessed data
"""

# To split the data into Train and Test
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,random_state= 0)

# To check the shape
print(X_train.shape) #The feature dataset for training.
print(y_train.shape) #The target variable for training.
print(X_test.shape)  #The feature dataset for testing.
print(y_test.shape)  #The target variable for testing.

"""# 7) Model Building

## A. Logistic Regression:
"""

# Features selection
X = df2[['Order', 'Member', 'Gender', 'Order Value', 'Age Group', 'Month']]
X = pd.get_dummies(X)
X.head()
y = df2.Churn

# Spliting the data into train and test data
X_train, X_test,y_train,y_test= train_test_split(X, y, train_size = 0.70, random_state = 30)

# Importing model
l_model= LogisticRegression()
l_model

# Train the model on training data
l_model.fit(X_train , y_train)

# Use the trained model to make predictions on new, unseen data.
y_pred = l_model.predict(X_test)
y_pred

# Display the first few rows of the true labels in the testing set
y_test.head()

# Confusion Matrix
confusion_matrix(y_pred,y_test)

# A function to evaluate the model
def evaluate_model(dt_classifier):
    print("Train Accuracy :", accuracy_score(y_train, dt_classifier.predict(X_train)))
    print("Train Confusion Matrix:")
    print(confusion_matrix(y_train, dt_classifier.predict(X_train)))
    print("-"*50)
    print("Test Accuracy :", accuracy_score(y_test, dt_classifier.predict(X_test)))
    print("Test Confusion Matrix:")
    print(confusion_matrix(y_test, dt_classifier.predict(X_test)))

# To evaluate the model
evaluate_model(l_model)

"""- As per the abbove evluation **Train accuracy is 64.92%** and **Test Accuracy is 66.62%**."""

# Getting accuracy score
print("Overall Accuracy: ", round(accuracy_score(y_pred, y_test),4)*100)

"""- The overall accuracy for Logistic Regression model is only **66.62%**. Which is not hat good."""

# Print the classification report
print(classification_report(y_test, y_pred))

"""### Inferences for Logistic Regression:
- This classification report is showing very poor Precision, Recall, and F1-Score for 'Not Churn (0)' whereas for 'Churned' data Precision is **67%** , Recall **100%**, and f1-Score is **80%**.
- However overall model accuracy is **67%**.
"""

# Get the predicted probabilities for class 1
y_pred_prob = l_model.predict_proba(X_test)[:, 1]

# Calculate the false positive rate, true positive rate, and thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

# Calculate the Area Under the Curve (AUC)
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.title('Receiver Operating Characteristic (ROC) Curve\n', fontdict={'fontsize':15,'fontweight':5,'color':'Green'})
plt.xlabel('False Positive Rate (FPR)\n', fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel('True Positive Rate (TPR)\n', fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})

plt.legend(loc='lower right')
plt.show()

"""### Inferences

- An AUC (Area Under the Curve) value of **53%** suggests that the performance of the logistic regression model is not very effective in distinguishing between the positive and negative classes. In general, an AUC value closer to 1 indicates better model performance, while a value closer to 0.5 suggests that the model's predictions are not significantly better than random chance.

- The logistic regression model is performing well for class 1 but poorly for class 0. It needs improvement in correctly identifying samples of class 0 (i.e., reducing false negatives) to achieve better overall performance.
Hence we can say logic reression model is not performing well for this sample data.

## B.  Random Forest

Random Forest Classifier is chosen for its ability to handle both numerical and categorical data, handle missing values, and reduce overfitting through bagging and feature randomness. It can handle large datasets with high-dimensional features and has high accuracy due to the ensemble of decision trees, making it a robust choice for classification tasks with complex relationships and multiple features.
"""

# Features selection
X = df2[['Order', 'Member', 'Gender', 'Order Value', 'Age Group', 'Month']]
X = pd.get_dummies(X)
X.head()
y = df2.Churn

# Spliting the data into train and test data
X_train, X_test,y_train,y_test = train_test_split(X, y,train_size = 0.7,random_state = 42)

# To import randomfforest
random = RandomForestClassifier(n_estimators =100)
random

# Fits the classifier to the input features (x_train) and their corresponding target labels (y_train)
random.fit(X_train,y_train)

# Use the trained model to make predictions on new, unseen data.
y_pred = random.predict(X_test)

# To print the confusion matrix
confusion_matrix(y_test,y_pred)

# Printing accuracy score
print("Overall Accuracy: ", round(accuracy_score(y_test,y_pred),2)*100)

# To print classification report
print(classification_report(y_test, y_pred))

"""### Inferences for Random Forest:

- The model shows good performance in correctly identifying churn cases (class 1) with high precision and recall. However, its performance on predicting not-churn cases (class 0) is not as high. The F1-score provides a balanced evaluation metric considering precision and recall for both classes.
- Overall, the model is reasonably performing good with **81% Accuracy**.
- For '**not-Churn**' data
   - Precision - **80%**
   - Recall - **60%**
   - F1-Score - **69%**
- For '**Churn**' data
   - Precision - **82%**
   - Recall - **92%**
   - F1-Score - **87%**

##   C.  Decision Tree Clasifier

- The decision tree classifier was chosen for the binary classification task due to its simplicity, interpretability, and ability to handle both numerical and categorical features. Decision trees create a tree-like model that recursively splits the data based on feature values, making it easy to understand the decision-making process.
- Additionally, decision trees can capture non-linear relationships and interactions between features, which can be valuable when dealing with complex datasets.
"""

# Features selection
X = df2[['Order', 'Member', 'Gender', 'Order Value', 'Age Group', 'Month']]
X = pd.get_dummies(X)
X.head()
y = df2.Churn

# Spliting features into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7 , random_state = 42)

# Importing the  model
classifier =  DecisionTreeClassifier()
classifier

# Fits the classifier to the input features (x_train) and their corresponding target labels (y_train)
classifier.fit(X_train, y_train)

# Use the trained model to make predictions on new, unseen data.
y_pred = classifier.predict(X_test)
y_pred

# To print confusion matrix
confusion_matrix(y_test,y_pred)

# Print accuracy score
print("Overall Accuracy: ", round(accuracy_score(y_test,y_pred),2)*100)

# To get classiffication report
print(classification_report(y_test, y_pred))

"""### Inferences for Decision Tree Classifier:
- Accuracy - **100%**
- Precision - **100%**
- Recall - **100%**
- F1-Score - **100%**
- As per the classification report Decision Tree Classifier model work extremmely well.
- A precision, recall, and F1-score of 1.00 for both classes (0 and 1) indicate a perfect classification performance on the test data. The accuracy of 1.00 means that the model made correct predictions for all the samples in the test set. Overall, the classifier is performing flawlessly with **100%** Accuracy, Precision, Recall, and F1-score, which is an ideal scenario.
"""

# Building decision Tree
from sklearn import tree
plt.figure(figsize=(20,15))
tree.plot_tree(classifier,fontsize=8)
plt.show()

"""#  D. Hyperparameter tuning using RandomizedSearch CV

- By performing hyperparameter tuning with RandomizedSearch CV, model can be fine-tune and can achieve better results.
- It allows to avoid the tedious and time-consuming process of manually selecting hyperparameters and automates the search process, improving the overall efficiency and effectiveness of the model training.
- When using RandomizedSearchCV for hyperparameter tuning, the results may vary slightly each time. The reason is that the algorithm randomly samples hyperparameter values from the specified search space and then performs cross-validation to evaluate the model's performance.
- To ensure reproducibility and consistent results across runs, we have set a random seed using the random_state parameter in the RandomizedSearchCV function.
- This will make the random sampling process deterministic and produce the same results in different runs with the same random seed.
"""

# Definig the parameter and  its values
n_estimators = [5,10,15,20,25,30,35,40,45,50,100]            # number of trees in the random forest
max_features = ['auto', 'sqrt','log2']                       # number of features in consideration at every split
max_depth = [int(k) for k in np.linspace(20, 150, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]     # minimum sample number to split a node
min_samples_leaf = [1, 3, 5, 7, 9]                           # minimum sample number that can be stored in a leaf node
bootstrap = [True, False]                                    # method used to sample data points
oob_score = [True, False]                                    # for oob to consider or not

random_grid = {'n_estimators': n_estimators,

'max_features': max_features,

'max_depth': max_depth,

'min_samples_split': min_samples_split,

'min_samples_leaf': min_samples_leaf,

'bootstrap': bootstrap}

# Define and create the Model
from sklearn.model_selection import RandomizedSearchCV
rfc_hyp = RandomForestClassifier()

# Randomsearch CV
rf_random = RandomizedSearchCV(estimator = rfc_hyp,param_distributions = random_grid,
               n_iter = 100, cv = 20, verbose=2, random_state=35, n_jobs = -1)

# To fit the train data
rf_random.fit(X_train, y_train)

"""n_iter: Number of parameter settings that are sampled.
    
cv: Number of cross-validation folds.
    
scoring: Evaluation metric to optimize (e.g., 'accuracy', 'f1', 'precision', 'recall', etc.).
"""

# Evaluate the Model
print ('Random grid: ', random_grid, '\n')

# Print the best parameters
print('Best Parameters: ', rf_random.best_estimator_.get_params(), '\n')

# Find the best combination of hyperparameters for the model that can improve its performance.
randmf = RandomForestClassifier(n_estimators=25, min_samples_split=4, min_samples_leaf=2, max_features='auto', max_depth=10, bootstrap=False)
randmf.fit(X_train, y_train)

# Use the trained model to make predictions on new, unseen data.
y_pred_test = randmf.predict(X_test)

# To print accuracy score
print("Overall Accuracy: ", round(accuracy_score(y_test, y_pred_test),2)*100)

# To print Classification report
print(classification_report(y_test, y_pred_test))

"""### Inferences for RandomizedSearch CV:
- Overall Model Accuracy - **79%**
- However, for '**Not-Churn**'data:
  - Precision - **83%**
  - Recall - **48%**
  - F1-Score - **61%**
- However, for '**Churn**':
  - Precision - **78%**
  - Recall - **95%**
  - F1-Score - **86%**

# E. XG BOOST

Gradient Boosting is an **ensemble learning method** that builds multiple weak learners (usually decision trees) sequentially. It works by fitting the weak learners to the errors made by the previous ones, thus reducing the overall error. The weak learners are combined to create a strong, robust, and accurate predictive model.
"""

# Initialize the model
XG_model = XGBClassifier()

# Fit a weak learner
XG_model.fit(X_train, y_train)

#Combine the newly fitted weak learner with the previous model to create an updated model.
y_pred = XG_model.predict(X_test)

# To get accuracy
print("Overall Accuracy: ", round(accuracy_score(y_test,y_pred),2)*100)

# To get Classification report
print(classification_report(y_test, y_pred))

"""### Inferences for XG Boost :
- Accuracy - **100%**
- Precision - **100%**
- Recall - **100%**
- F1-Score - **100%**

- The classification report shows that the Gradient Boosting model has achieved perfect accuracy (1.00) on both classes (0 and 1) in the test data. This means that the model is correctly predicting all instances of both classes, indicating a very good performance.

# F. Cross Validation Score

It is a function provided by scikit-learn that helps to perform k-fold cross-validation on a machine learning model. It is used to estimate the performance of a model by dividing the dataset into k subsets (folds). The model is trained on k-1 folds and tested on the remaining fold in each iteration, resulting in k evaluation scores.
"""

# Evaluating the model
cross_val_score(XG_model, X, y, cv=100).mean()

# Hard Voting
model = VotingClassifier(estimators=[("rf", random),  ("xgb", XG_model)], voting='hard')

# Printing score of hard voting
cross_val_score(model, X, y, cv=15).mean()

"""- A cross-validation **accuracy of 57.28%** with **hard voting** means that, on average, the majority vote of the models are 57.28% times correct."""

# Soft Voting
model = VotingClassifier(estimators=[("rf", random),  ("xgb", XG_model)], voting='soft')

# To print score of soft voting
cross_val_score(model, X, y, cv=10).mean()

"""### Inferences:
- A cross-validation accuracy of **91.27%** with soft voting means that, on average, the combined predictions of the models are correct **91.27%** of the time.
- Comparing the two methods, soft voting is performing significantly better than hard voting in this case. It suggests that considering the confidence of each model's prediction and combining them through averaging is resulting in better performance compared to the majority vote without considering confidence.

# 8 . CONCLUSION

## Based on the analysis and evaluation of different classification models, here are the conclusions:

- **Logistic Regression**: The logistic regression model achieved an accuracy of **67%**. It performed well in predicting class 1 (churned customers) with high precision, recall, and f1-score. However, it struggled with class 0 (retained customers) and showed lower precision, recall, and f1-score for this class.

- **Random Forest**: The random forest model achieved an accuracy of **81%**. It performed well in predicting both classes with high precision, recall, and f1-score. The model's ensemble nature helped in reducing overfitting, and it shows promise in making accurate predictions.

- **Decision Tree**: The decision tree model achieved an accuracy of **100%**. It showed perfect performance in both classes, achieving 100% precision, recall, and f1-score for both churned and retained customers. However, there is a possibility of overfitting.

- **HyperParameter tuning**: The Hyper tunned  model achieved an accuracy of **79%**. It performed well in predicting both classes with high precision, recall, and f1-score. Overall, the classifier is performing flawlessly with high accuracy, precision, recall, and F1-score, which is an good for a model.

- **Gradient Boosting**: The gradient boosting model achieved an accuracy of **100%**. Similar to the Decision Tree, it performed very well in predicting both classes with high precision, recall, and f1-score. The ensemble boosting technique improved the model's performance.

- **Soft Voting Ensemble**: The soft voting ensemble of different classifiers achieved an accuracy of **91.27%**. By combining the predictions of individual models, it improved overall performance and showed competitive results compared to individual models.

- In conclusion, the **Decision Tree**, **Gradient Boosting**, and **Soft Voting Ensemble** models showed impressive results in predicting churned and not-churn/retained customers. However, **Random Forest** also showed an overall good accuracy and better precision, recall, and f-score for both the categoreis.

# <font color = blue> Thanks for your patience  </font>
"""