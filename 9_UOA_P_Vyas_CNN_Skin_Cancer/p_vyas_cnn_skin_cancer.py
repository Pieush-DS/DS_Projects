# -*- coding: utf-8 -*-
"""Pieush_Vyas_CNN_Skin_Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17sFzpcQCudyMmys0oxgedniT4dyLCoc6

# <font color = blue>Multiclass classification model using a custom Convolutional Neural Network in tensorflow. </font>
### **Problem Statement:**
To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.

### Importing Skin Cancer Data
#### To do: Take necessary actions to read the data

### Importing all the important libraries
"""

import pathlib
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import PIL
import os
import shutil
from glob import glob
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

# If you are using the data by mounting the google drive, use the following :
from google.colab import drive
drive.mount('/content/gdrive')

##Ref:https://towardsdatascience.com/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166

# # For removing output folders
# pathrm = "/content/gdrive/MyDrive/Projects_to_be_done/CNN_Assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Train/"

# for folder in os.listdir(pathrm):
#   dirc = pathrm + folder + "/output"
#   shutil.rmtree(dirc)

"""This assignment uses a dataset of about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively."""

# Defining the path for train and test images
## Todo: Update the paths of the train and test dataset

train_dir= "/content/gdrive/MyDrive/Projects_to_be_done/CNN_Assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Train"
test_dir = "/content/gdrive/MyDrive/Projects_to_be_done/CNN_Assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Test"
data_dir_train = pathlib.Path(train_dir)
data_dir_test = pathlib.Path(test_dir)

image_count_train = len(list(data_dir_train.glob('*/*.jpg')))
print(image_count_train)
image_count_test = len(list(data_dir_test.glob('*/*.jpg')))
print(image_count_test)

"""### Load using keras.preprocessing

Let's load these images off disk using the helpful image_dataset_from_directory utility.

### Create a dataset

Define some parameters for the loader:
"""

batch_size = 32
img_height = 180
img_width = 180

"""Use 80% of the images for training, and 20% for validation."""

## Write your train dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset

train_ds = tf.keras.utils.image_dataset_from_directory(
              train_dir,
              batch_size=batch_size,
              subset='training',
              image_size=(img_height, img_width),
              seed=123,
              validation_split=0.2,
          )

## Write your validation dataset here
## Note use seed=123 while creating your dataset using tf.keras.preprocessing.image_dataset_from_directory
## Note, make sure your resize your images to the size img_height*img_width, while writting the dataset
val_ds = tf.keras.utils.image_dataset_from_directory(
              train_dir,
              batch_size=batch_size,
              subset='validation',
              image_size=(img_height, img_width),
              seed=123,
              validation_split=0.2,
          )

# List out all the classes of skin cancer and store them in a list.
# You can find the class names in the class_names attribute on these datasets.
# These correspond to the directory names in alphabetical order.
class_names = train_ds.class_names
print(class_names)

type(train_ds)

"""### Visualize the data
#### Todo, create a code to visualize one instance of all the nine classes present in the dataset
"""

import matplotlib.pyplot as plt


plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

"""The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.

`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.

`Dataset.prefetch()` overlaps data preprocessing and model execution while training.
"""

AUTOTUNE = tf.data.experimental.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

"""### Create the model
#### Todo: Create a CNN model, which can accurately detect 9 classes present in the dataset. Use ```layers.experimental.preprocessing.Rescaling``` to normalize pixel values between (0,1). The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network. Here, it is good to standardize values to be in the `[0, 1]`
"""

### Your code goes here

num_classes = 9

model1 = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes)
])

"""### Compile the model
Choose an appropirate optimiser and loss function for model training
"""

### Todo, choose an appropirate optimiser and loss function
model1.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model1.build([None,180,180,3])

# View the summary of all layers
model1.summary()

"""### Inferences:
- Total Trainable Parameters as per the above first model summary are 1,659,081.

### Train the model
"""

epochs = 20
history = model1.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""### Visualizing training results"""

# To visualize the training results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Accuracy",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.yticks(np.arange(0.0, 1, 0.15))
plt.xticks(np.arange(0, 22, 2))



plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.yticks(np.arange(0, 2.5, 0.25))
plt.xticks(np.arange(0, 22, 2))
plt.legend(loc='upper right')
plt.title('Training and Validation Loss\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Loss",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit

### Inferences:
- Here, it is clearly visible that there is an overfit as validation loss is increasing and training loss is constantly decreasing.
- However, Training Accuuracy is **0.82**, validation loss is **2.04**, and val_accurracy is **0.4765**.

### Write your findings here
"""

# Todo, after you have analysed the model fit history for presence of underfit or overfit, choose an appropriate data augumentation strategy.
# Your code goes here

data_augmentation = tf.keras.Sequential([
    layers.experimental.preprocessing.RandomFlip("horizontal_and_vertical"),
    layers.experimental.preprocessing.RandomRotation((-0.2, 0.3))
])

# Todo, visualize how your augmentation strategy works for one instance of training image.
# Your code goes here

plt.figure(figsize=(10,10))
for images, _ in train_ds.take(1):
  for i in range(9):
    aug_image = data_augmentation(images)
    ax = plt.subplot(3,3,i+1)
    plt.imshow(aug_image[0].numpy().astype("uint8"))
    plt.axis('off')

"""### Todo:
### Create the model, compile and train the model

"""

## You can use Dropout layer if there is an evidence of overfitting in your findings

## Your code goes here

num_classes = 9

model2 = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),
  data_augmentation,
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Dropout(.2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes)
])

"""### Compiling the model"""

## Your code goes here

model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""### Training the model"""

## Your code goes here, note: train your model for 20 epochs

epochs = 20
history = model2.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""### Visualizing the results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Accuracy",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.yticks(np.arange(0, 1, 0.15))
plt.xticks(np.arange(0, 22, 2))

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Loss",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.yticks(np.arange(0, 2.5, 0.25))
plt.xticks(np.arange(0, 22, 2))
plt.show()

"""#### Todo: Write your findings after the model fit, see if there is an evidence of model overfit or underfit. Do you think there is some improvement now as compared to the previous model run?

### Inferences:
- Here, we can see that there is negligible overfit.
- Training Accuuracy is **0.599**, validation loss is **1.27**, and val_accurracy is **0.559**.

- We can see that the validation loss is less than before **(2.03 vs 1.27)**.

#### **Todo:** Find the distribution of classes in the training dataset.
#### **Context:** Many times real life datasets can have class imbalance, one class can have proportionately higher number of samples compared to the others. Class imbalance can have a detrimental effect on the final model quality. Hence as a sanity check it becomes important to check what is the distribution of classes in the data.
"""

## Your code goes here.


class_count = {}

for dir in os.listdir(train_dir):
  class_count[dir] = len(os.listdir(train_dir +'/'+ dir))
class_count

"""#### **Todo:** Write your findings here:
#### - Which class has the least number of samples?
#### - Which classes dominate the data in terms proportionate number of samples?

"""

print("class with least number of samples:",min(class_count, key=class_count.get))
print("class with most number of samples:",max(class_count, key=class_count.get))

"""### Inferences:
- After analysis we can say that **seborrheic keratosis** has **least number** of samples.
- Whereas **pigmented benign keratosis** class **dominate** the data with having maximum number of samples.

#### **Todo:** Rectify the class imbalance
#### **Context:** You can use a python package known as `Augmentor` (https://augmentor.readthedocs.io/en/master/) to add more samples across all classes so that none of the classes have very few samples.
"""

!pip install Augmentor

"""To use `Augmentor`, the following general procedure is followed:

1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>
2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>
3. Execute these operations by calling the `Pipelineâ€™s` `sample()` method.

"""

path_to_training_dataset = train_dir + '/'
import Augmentor
for i in class_names:
    p = Augmentor.Pipeline(path_to_training_dataset + i)
    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)
    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.

"""Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images."""

image_count_train = len(list(data_dir_train.glob('*/output/*.jpg')))
print(image_count_train)

"""### Lets see the distribution of augmented data after adding new images to the original training data."""

path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]
path_list

lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]
lesion_list_new

dataframe_dict_new = dict(zip(path_list, lesion_list_new))

df2 = pd.DataFrame(list(dataframe_dict_new.items()),columns = ['Path','Label'])
# new_df = original_df.append(df2)

df2['Label'].value_counts()

"""- So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process.

#### **Todo**: Train the model on the data created using Augmentor
"""

batch_size = 32
img_height = 180
img_width = 180

"""#### **Todo:** Create a training dataset"""

data_dir_train="/content/gdrive/MyDrive/Projects_to_be_done/CNN_Assignment/Skin cancer ISIC The International Skin Imaging Collaboration/Train"
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir_train,
  seed=123,
  validation_split = 0.2,
  subset = "training",
  image_size=(img_height, img_width),
  batch_size=batch_size)

"""#### **Todo:** Create a validation dataset"""

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir_train,
  seed=123,
  validation_split = 0.2,
  subset = "validation",
  image_size=(img_height, img_width),
  batch_size=batch_size)

"""#### **Todo:** Create your model (make sure to include normalization)"""

## your code goes here

num_classes = 9

model3 = tf.keras.Sequential([
  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(num_classes)
])

"""#### **Todo:** Compile your model (Choose optimizer and loss function appropriately)"""

## your code goes here

model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""#### **Todo:**  Train your model"""

# epochs = 30
## Your code goes here, use 30 epochs.
epochs = 30
history = model3.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""#### **Todo:**  Visualize the model results"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(14, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Accuracy",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.yticks(np.arange(0, 1, 0.15))
plt.xticks(np.arange(0, 33, 3))


plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss\n',fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("Epochs",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Loss",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.yticks(np.arange(0, 2.5, 0.25))
plt.xticks(np.arange(0, 33, 3))

plt.show()

"""#### **Todo:**  Analyze your results here. Did you get rid of underfitting/overfitting? Did class rebalance help?

### Inferences:
- Here, we can see that we got rid of overfitting as validation and training acccuracy both has been increased with reduced validation loss.
- However, Training Accuuracy is approx. **0.95**, validation loss is **0.83**, and val_accurracy is **0.83** at 29th epoch.
"""

# Here we can finally see that the undefitting is significantly reduced as the validation loss descresed.

"""###Summary

- **Model-1** has large gap between training data and validation data, therefore there is a overfitting. However, Validation accuracy is around 47%.

- **Model-2** tried removing overfitting using data augmentation. Reduced gap but validation accuracy improvement is not that much significant(around 56%) because of imbalanced data.

- **Model-3** dealt with imbalanced data and finally we were able to achieve good validation accuracy which is around **83%**.

**Conclusion**

- As per the above analysis a final model-3 is a good model for our problem statement. A Dermatologists can timely detect and do the analysis by evaluating images of the Melanoma (a type of cancer) therefore, number of deaths can be reduced. Such in time and accurate analysis reduces a lot of manual efforts which are required in diagnosis.

# End of Assignment
"""

