# -*- coding: utf-8 -*-
"""Pieush_Advance+Regression_US_House.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bpa7kbcc-ucoS6kMwev1K2HyfW8CGHAW

# Advance Regression using Regularisation
### Problem Statement:
A US based housing company named 'Surprise Housing' has decided to enter the Australian market. The company users data analytics to purchase houses at a price below their actual values and flip them on at a higher price.

The company is looking at prospective properties to buy to enter the market, therefore required to build a model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.

The company want to know the following things about the prospective properties:

A) Which variables are significant in predicting the price of a house,

B) How well those variables describe the price of a house.

### Business Goal:
Required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. Accordingly they can manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market.

#### Steps to Follow...
1. Reading and Understanding Data

2. Cleaning of the Dataset

3. Visualising the Data

4. Data Preparation

5. Splitting the Data into Training and Testing Sets

6. Feature Scaling

7. Building the Model

8. Residual Analysis of the train data

9. Making predictions using final model

10. Finding Optimal Cut-Off Point.

11. Predictions and Model evaluation on Test data

12. Assigning Lead Score

#### Calling various essential library functions
"""

# Suppressing Warnings
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# Importing the required libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from scipy import stats
import statsmodels
import statsmodels.api as sm

import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from statsmodels.stats.outliers_influence import variance_inflation_factor #Check for the VIF values of the feature variables.
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import classification_report
from sklearn.neighbors import KNeighborsClassifier

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso

# %matplotlib inline

# To set the Row, Column and Width of the Dataframe to show on Jupyter Notebook
pd.set_option('display.max_rows', 10000)
pd.set_option('display.max_columns', 2000)
pd.set_option('display.width', 1000)

"""### Step 1: Importing and Reading the Data"""

# Importing all datasets
us_housing = pd.read_csv("F:/Upgrade/Course-3 (ML_II)\Advance Regression_Assignment/train.csv")

# To check all the variables label
us_housing.head()

"""### Step 2: Inspecting the Dataframe   
Inspect the dataframe for dimensions, null-values, and summary of different numeric columns.
"""

# To Check the dimensions of the dataframe
us_housing.shape

# To see the statistical aspects of the dataframe
us_housing.describe()

# To see the type of each column
us_housing.info()

# To check the null values of each column
us_housing.isnull().sum()

# Checking the percentage of missing values
us_housing_null_per = round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)
us_housing_null_per

# To check any duplicate value in various columns
us_housing.duplicated().sum()

"""### Step 3: Data Cleaning"""

# To check number of columns which has Null values greater than 40%
len(us_housing_null_per[us_housing_null_per.values>=(40)])

# List all the columns which values is greater than and equal to 40%
us_housing_null_per_40 = list(us_housing_null_per[us_housing_null_per.values>=40].index)
us_housing_null_per_40

# Drop all the values which values is greater than equal to 40%.
us_housing_not_null = us_housing.drop(labels = us_housing_null_per_40, axis=1, inplace=True)

# To check the no. of rows and columns in the given dataframe
us_housing.shape

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To check the details of the Null column with all the rows
us_housing[us_housing['LotFrontage'].isnull()]

# To do value counts
us_housing['LotFrontage'].value_counts(dropna=False)

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing["LotFrontage"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing['LotFrontage'].describe(),2)

"""#### Value of Q1(25%), Q2(50%), Q3(75%)
Q1 = 59.00,
Q2 = 69.00,
Q3 = 80.00, whereas mean of it is 70.05

"""

# Find out the quantile ( 0, 0.25, 0.5, 0.7, 0.75, 0.8, 0.9, 0.95, 0.99, 1) of the respective column
us_housing["LotFrontage"].quantile([0, 0.25, 0.5, 0.7, 0.75, 0.8, 0.9, 0.95, 0.99, 1])

"""##### As this column has outliers we should replace the null/ NaN values by median of it."""

# To get string value which occur maximum in the column
LotFr_M = round(us_housing['LotFrontage'].median(),2)
LotFr_M

# Replace or fill null values of the respective column with the maximum occured value in the column.
us_housing['LotFrontage'] = us_housing['LotFrontage'].fillna(LotFr_M)

# To recalculate the null value of the variable
us_housing['LotFrontage'].isnull().sum()

# To see the statistical aspects of the dataframe
round(us_housing['LotFrontage'].describe(),2)

"""##### Value of Q1(25%), Q2(50%), Q3(75%)
Q1 = 60.00,
Q2 = 69.00,
Q3 = 79.00, whereas mean of it is 69.86

##### Outlier Treatment
"""

# To find out value of IQR
LotFr_IQR = stats.iqr(us_housing["LotFrontage"], interpolation = 'midpoint')
LotFr_IQR

# Find the suspected outliers Q1 - 1.5*IQR
Neg_Out = 60.00 - (1.5 * LotFr_IQR)
Neg_Out

# Find the suspected outliers Q3 + 1.5*IQR
Sus_Pos_Out = 79.00 + (1.5 * LotFr_IQR)
Sus_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Con_Pos_Out = 79.00 + (3 * LotFr_IQR)
Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing = us_housing[~((us_housing['LotFrontage'] < Neg_Out) | (us_housing['LotFrontage'] > Con_Pos_Out))]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing["LotFrontage"])
plt.show()

# To check the shape of the dataframe
us_housing.shape

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['MasVnrType'].describe()

# To do value counts
us_housing['MasVnrType'].value_counts(dropna=False)

# To get string value which occur maximum in the column
MasVnr_M = us_housing['MasVnrType'].mode()[0]
MasVnr_M

# Replace or fill null values of the respective column with the maximum occured value in the column.
us_housing['MasVnrType'] = us_housing['MasVnrType'].fillna(MasVnr_M)

# To recheck Null values
us_housing['MasVnrType'].isnull().sum()

# To do value counts
round(us_housing['MasVnrArea'].describe(), 2)

# To do value counts
us_housing['MasVnrArea'].value_counts(dropna=False)

# To get string value which occur maximum in the column
MasVnrA_M = us_housing['MasVnrArea'].mode()[0]
MasVnrA_M

# Replace or fill null values of the respective column with the maximum occured value in the column.
us_housing['MasVnrArea'] = us_housing['MasVnrArea'].fillna(MasVnrA_M)

# To recheck Null values
us_housing['MasVnrArea'].isnull().sum()

"""##### Outliers Treatment for the respective column"""

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing["MasVnrArea"])
plt.show()

# To do value counts
round(us_housing['MasVnrArea'].describe(), 2)

"""##### Value of Q1(25%), Q2(50%), Q3(75%)
Q1 = 0.00,
Q2 = 0.00,
Q3 = 162.25, whereas mean of it is 102.02

"""

# To find out value of IQR
MasVnrA_IQR = stats.iqr(us_housing["MasVnrArea"], interpolation = 'midpoint')
MasVnrA_IQR

# # Find the suspected outliers Q1 - 1.5*IQR
# Mas_Neg_Out = 0.00 - (1.5 * LotFr_IQR)
# Mas_Neg_Out

# Find the suspected outliers Q3 + 1.5*IQR
Mas_Sus_Pos_Out = 162.25 + (1.5 * LotFr_IQR)
Mas_Sus_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Mas_Con_Pos_Out = 162.25 + (3 * LotFr_IQR)
Mas_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing = us_housing[us_housing['MasVnrArea'] <= Mas_Con_Pos_Out]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing["MasVnrArea"])
plt.show()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To check the shape of the dataframe
us_housing.shape

# To do value counts
us_housing['Electrical'].describe()

# To do value counts
us_housing['Electrical'].value_counts(dropna=False)

# To get string value which occur maximum in the column
Electr_M = us_housing['Electrical'].mode()[0]
Electr_M

# Replace or fill null values of the respective column with the maximum occured value in the column.
us_housing['Electrical'] = us_housing['Electrical'].fillna(Electr_M)

# To recheck Null values
us_housing['Electrical'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['BsmtQual'].describe()

# To do value counts
us_housing['BsmtQual'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtQual analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['BsmtQual'] = us_housing['BsmtQual'].replace(np.NaN, 'NA')

# To recheck the Null values
us_housing['BsmtQual'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['BsmtCond'].describe()

# To do value counts
us_housing['BsmtCond'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtQual analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['BsmtCond'] = us_housing['BsmtCond'].replace(np.NaN, 'NA')

# To recheck the Null values
us_housing['BsmtQual'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['BsmtExposure'].describe()

# To do value counts
us_housing['BsmtExposure'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtQual analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['BsmtExposure'] = us_housing['BsmtExposure'].replace(np.NaN, 'NA')

# To recheck the Null values
us_housing['BsmtExposure'].isnull().sum()

# To do value counts
us_housing['BsmtFinType1'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['BsmtFinType1'] = us_housing['BsmtFinType1'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['BsmtFinType1'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['BsmtFinType2'].describe()

# To do value counts
us_housing['BsmtFinType2'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['BsmtFinType2'] = us_housing['BsmtFinType2'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['BsmtFinType2'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To see the statistical aspects of the dataframe
us_housing['GarageType'].describe()

# To do value counts
us_housing['GarageType'].value_counts(dropna=False)

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['GarageType'] = us_housing['GarageType'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['GarageType'].isnull().sum()

# To do value counts
us_housing['GarageYrBlt'].value_counts(dropna=False)

# To see the statistical aspects of the dataframe
round(us_housing['GarageYrBlt'].describe(), 2)

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing["GarageYrBlt"])
plt.show()

"""###### There are no outliers, therefore we can take mean"""

# To get string value which occur maximum in the column
GargBlt_M = round(us_housing['GarageYrBlt'].mean())
GargBlt_M

# Replace or fill null values of the respective column with the maximum occured value in the column.
us_housing['GarageYrBlt'] = us_housing['GarageYrBlt'].fillna(GargBlt_M)

# To recheck Null values
us_housing['GarageYrBlt'].isnull().sum()

# To do value counts
us_housing['GarageFinish'].value_counts(dropna=False)

# To see the statistical aspects of the dataframe
us_housing['GarageFinish'].describe()

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['GarageFinish'] = us_housing['GarageFinish'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['GarageFinish'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To do value counts
us_housing['GarageQual'].value_counts(dropna=False)

# To see the statistical aspects of the dataframe
us_housing['GarageQual'].describe()

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['GarageQual'] = us_housing['GarageQual'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['GarageQual'].isnull().sum()

# To do value counts
us_housing['GarageCond'].value_counts(dropna=False)

# To see the statistical aspects of the dataframe
us_housing['GarageCond'].describe()

# To replace 'NaN' with 'NA' as BsmtFinType1 analyze the Height of the basement and 'NA' represents no basements which may not change the result.
us_housing['GarageCond'] = us_housing['GarageCond'].replace(np.NaN, 'NA')

# To recheck Null values
us_housing['GarageCond'].isnull().sum()

# Checking the percentage of missing values
round(100*(us_housing.isnull().sum()/len(us_housing.index)), 2)

# To check the shape of the Dataframe
us_housing.shape

# To see the type of each column
us_housing.info()

# To check all the variables label
us_housing.head()

# To convert 'MSSubClass' column data type from int to object as type of Dwelling should be object instead of number
us_housing.MSSubClass = us_housing.MSSubClass.astype("object")

# To see the type of each column
us_housing.info()

# To see if the 'SalePrice' column is normally distributed or not
fig = plt.figure(figsize=(7,5))
sns.distplot(us_housing['SalePrice'])
plt.title("Housing Sale Price\n", fontdict={'fontsize':16,'fontweight':5,'color':'Green'})
plt.xlabel("\nSale Price",fontdict={'fontsize':14,'fontweight':5,'color':'Brown'})
plt.ylabel("Sale Density\n",fontdict={'fontsize':14,'fontweight':5,'color':'Brown'})
plt.show()

"""##### Inferences
- As per the above graph 'Sale Price' is normally distributed.

#### To analyze other variables.
"""

# To check no. of columns
us_housing.columns

"""##### Outliers' Analysis and Treatment"""

# Describe the numerical columns of the dataframe
num_housing = round(us_housing.describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]),2)
num_housing

# Size of numerical variables
num_housing.columns.shape

# Shape of the dataframe
us_housing.shape

"""###### As per the above shape there are 37 numerical data out of that one is 'id' (not usable column) and 1 is the reference variable, therefore in total there are 36 numerical variables, and out of 76, 39 are categorical variables.

###### Target Variable  - 1 (One) - 'SalePrice'

###### Numeric Variables- 35 (Thirty Five) -
['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',
 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',
'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',
 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',
 'MiscVal', 'MoSold', 'YrSold']
                    
###### Categorical Variables- 39 (Thirty Nine)
['MSSubClass', 'MSZoning','Street', 'LotShape', 'LandContour', 'Utilities',
'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle','RoofStyle', 'RoofMatl',
'Exterior1st', 'Exterior2nd', 'MasVnrType','ExterQual', 'ExterCond', 'Foundation','BsmtQual', 'BsmtCond', 'BsmtExposure',
'BsmtFinType1','BsmtFinType2','Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive','SaleType', 'SaleCondition']
"""

# Create new dataframe from previous one.
us_housing_data = us_housing.copy(deep=True)

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["LotArea"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['LotArea'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find out value of IQR
LotAr_IQR = stats.iqr(us_housing_data["LotArea"], interpolation = 'midpoint')
LotAr_IQR

# Find the suspected outliers Q1 - 1.5*IQR
NegAr_Out = 7486.00 - (1.5 * LotAr_IQR)
NegAr_Out

# Find the suspected outliers Q3 + 1.5*IQR
SusAr_Pos_Out = 11215.00 + (1.5 * LotAr_IQR)
SusAr_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Ar_Con_Pos_Out = 11215.00 + (3 * LotAr_IQR)
Ar_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing_data = us_housing_data[~((us_housing_data['LotArea'] < NegAr_Out) | (us_housing_data['LotArea'] > Ar_Con_Pos_Out))]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["LotArea"])
plt.show()

# To check the shape
us_housing_data.shape

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["BsmtFinSF1"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['BsmtFinSF1'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["BsmtFinSF2"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["BsmtUnfSF"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['BsmtUnfSF'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["TotalBsmtSF"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['TotalBsmtSF'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find out value of IQR
TBsmtF_IQR = stats.iqr(us_housing_data["TotalBsmtSF"], interpolation = 'midpoint')
TBsmtF_IQR

# Find the suspected outliers Q1 - 1.5*IQR
NegTBs_Out = 784.00 - (1.5 * TBsmtF_IQR)
NegTBs_Out

# Find the suspected outliers Q3 + 1.5*IQR
TBsmtF_Pos_Out = 1205.75 + (1.5 * TBsmtF_IQR)
TBsmtF_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
TBsmtF_Con_Pos_Out = 1205.75 + (3 * TBsmtF_IQR)
TBsmtF_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing_data = us_housing_data[~((us_housing_data['TotalBsmtSF'] < NegTBs_Out) | (us_housing_data['TotalBsmtSF'] > TBsmtF_Con_Pos_Out))]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["TotalBsmtSF"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["LowQualFinSF"])
plt.show()

# To check the shape of the Dataframe
us_housing_data.shape

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["WoodDeckSF"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['WoodDeckSF'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find out value of IQR
Wood_IQR = stats.iqr(us_housing_data["WoodDeckSF"], interpolation = 'midpoint')
Wood_IQR

# Find the suspected outliers Q3 + 1.5*IQR
Wood_Pos_Out = 157.50 + (1.5 * Wood_IQR)
Wood_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Wood_Con_Pos_Out = 157.50 + (3 * Wood_IQR)
Wood_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing_data = us_housing_data[us_housing_data['WoodDeckSF'] <= Wood_Con_Pos_Out]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["WoodDeckSF"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["OpenPorchSF"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['OpenPorchSF'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find out value of IQR
Open_IQR = stats.iqr(us_housing_data["OpenPorchSF"], interpolation = 'midpoint')
Open_IQR

# Find the suspected outliers Q3 + 1.5*IQR
Open_Pos_Out = 60.75 + (1.5 * Open_IQR)
Open_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Open_Con_Pos_Out = 60.75 + (3 * Open_IQR)
Open_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing_data = us_housing_data[us_housing_data['OpenPorchSF'] <= Open_Con_Pos_Out]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["OpenPorchSF"])
plt.show()

# To check the shape of the dataframe
us_housing_data.shape

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["EnclosedPorch"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["3SsnPorch"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["ScreenPorch"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["PoolArea"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["MiscVal"])
plt.show()

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["SalePrice"])
plt.show()

# To see the statistical aspects of the dataframe
round(us_housing_data['SalePrice'].describe(percentiles=[0.25, 0.5, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]), 2)

# To find out value of IQR
Sale_IQR = stats.iqr(us_housing_data["SalePrice"], interpolation = 'midpoint')
Sale_IQR

# Find the suspected outliers Q3 + 1.5*IQR
Sale_Pos_Out = 194000.00 + (1.5 * Sale_IQR)
Sale_Pos_Out

# Find the confirmed outlier Q3 + (3 * IQR)
Sale_Con_Pos_Out = 194000.00 + (3 * Sale_IQR)
Sale_Con_Pos_Out

# To drop outliers after confirmed outliers
us_housing_data = us_housing_data[us_housing_data['SalePrice'] <= Sale_Con_Pos_Out]

# To find outliers for the respective column
plt.figure(figsize = [10,3])
sns.boxplot(us_housing_data["SalePrice"])
plt.show()

"""#### Analyse Yes/No variables"""

# To do vale counts on the respective column
us_housing_data['CentralAir'].value_counts(dropna=False)

"""##### As Few categorical columns have more than 90% of 'Yes' and some has too much outliers therefore we need to remove those unnecessary columns as they may not able to give much informations."""

# Create new dataframe from previous one.
us_housing_clean = us_housing_data.copy(deep=True)

us_housing_clean.shape

"""###### To Drop various unnecessary columns which has too much outliers"""

# To drop various unnecessary columns or columns which has too much outliers
us_housing_clean.drop(["Id","BsmtFinSF2","LowQualFinSF","EnclosedPorch","3SsnPorch","ScreenPorch","PoolArea","MiscVal","CentralAir"], axis=1, inplace=True)

# To check column names
us_housing_clean.shape

# To visualize coorrelation between various data with Heatmap
fig = plt.figure(figsize=(30,20))
us_cor = us_housing_clean.corr()
sns.heatmap(us_cor, cmap="RdYlGn", annot = True )
plt.title("Correlation between various Numercial Varibales\n", fontdict={'fontsize':24,'fontweight':8,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':20,'fontweight':8,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':20,'fontweight':8,'color':'Brown'})
plt.show()

# To collect and see absolute coorelation matrix
cor_matrix = us_cor.abs()
cor_matrix

# To find out Upper Trainangle
upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))
upper_tri

# To fid out the features which are highly correlated
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.80)]
to_drop

"""###### As per the analysis '1stFlrSF', 'TotRmsAbvGrd', and 'GarageArea' columns are highly correlated with each other therefore need to drop these variables.

"""

# To drop the variables which are highly correlated.
us_housing_clean.drop(['1stFlrSF', 'TotRmsAbvGrd', 'GarageArea'], axis=1, inplace=True)

# Shape of the dataframe
us_housing_clean.shape

# To visualize coorrelation between various data with Heatmap
fig = plt.figure(figsize=(30,20))
us_cor1 = us_housing_clean.corr()
sns.heatmap(us_cor1, cmap="RdYlGn", annot = True )
plt.title("Correlation between various Numercial Varibales\n", fontdict={'fontsize':24,'fontweight':8,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':20,'fontweight':8,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':20,'fontweight':8,'color':'Brown'})
plt.show()

# To see various values of correlations in %
round(us_cor1*100,2)

# To check the no. of rows and columns in the given dataframe
us_housing_clean.shape

# To get details for various variables of the dataframe
us_housing_clean.info()

# To seperate categorical data from Numerical data
# Housing_Categorical = us_housing_clean.loc[:,us_housing_clean.dtypes==np.object]
Housing_Categorical = us_housing_clean.select_dtypes(exclude=['object'])

# To check head of the dataframe
Housing_Categorical.head()

# To check the shape
Housing_Categorical.shape

# To check columns name
Housing_Categorical.columns

"""##### Distribution of Data in the final Dataframe
- Total Variables: 64
- Categorical Variables: 38
- Integer Variables: 25 + 1 (SalePrice)

"""

# To check the shape
us_housing_clean.shape

"""### Step 4: Data Visualization

#### A) Univariate Analysis
"""

# To see variable heads
us_housing_clean.head()

# To collect all the numerical variables
us_num_list = us_housing_clean.select_dtypes(exclude=['object'])
us_num_list.head()

# To analyze Lead Source columns
fig = plt.figure(figsize=(9,7))
us_housing['MSSubClass'].value_counts().plot.barh()
plt.title("Type of Dwelling Vs Count\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Type of Dwellings\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences:
- As per the above graph it is clear that the Top 3 Dwellings are of "1-STORY 1946 & NEWER ALL STYLES", "2-STORY 1946 & NEWER", "1-1/2 STORY FINISHED ALL AGES" which represents 20, 60 and 50 respectively.
- Whereas the last two minimum types of Dwellings are "1-STORY W/FINISHED ATTIC ALL AGES" and "PUD - MULTILEVEL - INCL SPLIT LEV/FOYER" which are representing 40, 180 respectively.
"""

# To analyze Lead Source
fig = plt.figure(figsize=(9,7))
us_housing['MSZoning'].value_counts().plot.barh()
plt.title("Zoning classifciations of Sale Vs Count\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Various Zones of Sale\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences:
- Among various Zones of Sale the top two Zones are "Residential Low Density" and "Residential Medium Density".
- Whereas the less saling zone is "Commercial" Zone.
"""

# To analyze Last Activity Column
fig = plt.figure(figsize=(9,7))
us_housing['Street'].value_counts().plot.barh()
plt.title("Type of Road Access\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Type of Road Access\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""### Inferences:
- The maximum available type of Road Access to the property is "Paved".

"""

# To analyze 'LotShape' Column
fig = plt.figure(figsize=(9,7))
us_housing['LotShape'].value_counts().plot.barh()
plt.title("Shape of Property\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Shape of Property\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences:
- As per the above graph it is clear that the top most General shape of property is "Regular"
- Whereas the least available shape of the property is "Irregular".

"""

# To analyze 'LandContour' columns
fig = plt.figure(figsize=(9,7))
us_housing['LandContour'].value_counts().plot.barh()
plt.title("Property Flatness\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Property Flatness\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences:
- As per the above graph it is clear that the maximum avaialble type of "Flatness of the property" is the "Near Flat/Level".
-  Whereas the least available type is "Depression".

"""

# To analyze 'Utility' columns
fig = plt.figure(figsize=(9,7))
us_housing['Utilities'].value_counts().plot.barh()
plt.title("Types of Utilities\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Types of Utilities\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

# To check value counts
us_housing_clean['Utilities'].value_counts(dropna=False)

"""#### Inferences
- The only avaiable or more than 90% available type of Utility is "All public Utilities (E,G,W,& S)".

"""

# To analyze 'Neighnorhood' columns
fig = plt.figure(figsize=(9,7))
us_housing_clean['Neighborhood'].value_counts().plot.barh()
plt.title("Physical Location within Ames City\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Physical Location within Ames City\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""##### Inferences
- As per the above graph it is clear that the top three 'Physical Location within Ames City' are "NAmes", "CollgCr", and "OldTown" respectively.
- Whereas the minimum available location is "Northpark Villa".

"""

# To analyze 'HouseStyle' columns
fig = plt.figure(figsize=(9,7))
us_housing_clean['HouseStyle'].value_counts().plot.barh()
plt.title("Style of dwelling\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Style of dwelling\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences:
- As per the above graph that the top most Style of Dwelling is "1Story".
- Whereas the least available style is of Dwelling is "Two and one-half story: 2nd level finished".

"""

# To analyze 'OverallQual' columns
fig = plt.figure(figsize=(9,7))
us_housing_clean['OverallQual'].value_counts().plot.barh()
plt.title("Overall Quality\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Overall Quality\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences
- As per the above graph the "Rates of the overall material and finish of the house" are mostly Average and above average.
- Whereas Poor and Very Poor are very minimum.

"""

# To analyze 'OverallCond' columns
fig = plt.figure(figsize=(9,7))
us_housing_clean['OverallCond'].value_counts().plot.barh()
plt.title("Overall Condition\n", fontdict={'fontsize':20,'fontweight':5,'color':'Green'})
plt.xlabel("\nCounts\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("Overall Condition\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.show()

"""#### Inferences
- As per the above graph most of the "Rates the overall condition of the houseleads" are Average and Above Average.
- Whereas the Poor and Very Poor are the lowest overall condition.

"""

# To check the name of columns
us_housing_clean.columns

"""#####  B) Bivariate/Multivariate Analysis"""

# To plot the pairplot among various variables

fig = plt.figure(figsize=(30,500))

j=1
for i in us_num_list:
    plt.subplot(70, 3, j)
    sns.scatterplot(x = us_housing_clean[i], y = us_housing_clean['SalePrice'], data = us_housing_clean)
    j=j+1;

plt.tight_layout()
plt.show()

"""##### Inferences
- As per the above pair plot 'Sale Price' is highly correlated with 'TotalBsmtSF', '2ndFlrSF', and 'GrLivArea', therefore as these variables respective value increases Sale Prices increases.

### Step 5: Data Preparation

#### Preparing the Data for Modeling.

- Encoding
  - Converting Categorical variables to dummy variables
"""

# To see shape of the dataframe
us_housing_clean.shape

# Create new dataframe from previous one.
us_housing_clean_1 = us_housing_clean.copy(deep=True)

# Create new dataframe from previous one.
X_us_housing = us_housing_clean_1.copy(deep=True)

# To check the heads of the variable
X_us_housing.head()

# To check the Null values
X_us_housing.isnull().sum()

# To get data types of the variables.
X_us_housing.info()

"""#### To create Dummy Variables.

- All the categorical variables has few levels and to convert these levels into integer we will use `dummy variables`.
"""

# To collect all the categorical variables
us_house_cat = X_us_housing.select_dtypes(include=['object'])
us_house_cat.head()

# To check the shape of the dataframe
us_house_cat.shape

# To collect all the numerical variables
us_house_num = X_us_housing.select_dtypes(exclude=['object'])
us_house_num.head()

# To check the shape of the dataframe
us_house_num.shape

# convert into dummies
us_house_dummies = pd.get_dummies(us_house_cat, drop_first=True)
us_house_dummies.head()

# To drop main categorical variables as dummy variables has been created
X_us_housing = X_us_housing.drop(list(us_house_cat.columns), axis=1)

# To concat dummy variables with other variables of the dataframe
X_us_housing = pd.concat([X_us_housing, us_house_dummies], axis=1)

# To check the heads of the dataframe
X_us_housing.head()

# To check the shape of the dataframe
X_us_housing.shape

# To check the null values
X_us_housing.isnull().sum()

"""### Step 6: Splitting the Data into Training and Testing Sets

- The very first basic step for regression is performing a train-test split.

"""

# Create new dataframe from previous one.
X_housing = X_us_housing.copy(deep=True)

# To check the shape
X_housing.shape

# Putting feature variable to X
X = X_housing.drop(['SalePrice'], axis=1)
X.head()

# To check the shape
X.shape

# Putting response variable to y
y = X_housing['SalePrice']
y.head()

# To check the shape
y.shape

# Splitting the data into train and test
np.random.seed(0)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)

# To check the shape of the Train data
X_train.shape

# To check the shape of the Test data
X_test.shape

"""### Step 7: Rescaling the Features

In Simple Linear Regression, scaling doesn't impact. When variable has little different interger values then rescale the variables to make them comparable. Without comparable scales, some of the coefficients obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This may be very critical during model evaluation. Therefore standardization or normalization should be used so that the units of all the coefficients would be on the same scale. There are two types of rescaling:

1. Standardisation (mean-0, sigma-1)
2. Min-Max scaling : Between 0 and 1

Here, I will use Standardizing scaling.

"""

# To check all the variables
X_housing.head()

# To check the shape of the Dataframe
X_housing.shape

# to see the numerical variables
us_house_num.head()

# To check the shape of the numerical variables.
us_house_num.shape

# To check the name of the columns
us_house_num.columns

# To Create List of variables

# Apply scaler() to all the columns except the 'dummy' variables
X_housing_num_vars = ['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',
                      'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',
                      'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageYrBlt', 'GarageCars',
                      'WoodDeckSF', 'OpenPorchSF', 'MoSold', 'YrSold']

# Do scaling on the variables
X_housing_scaler = StandardScaler()

# To Fit all the Train data
X_train[X_housing_num_vars] = X_housing_scaler.fit_transform(X_train[X_housing_num_vars])

# To Fit all the Test data
X_test[X_housing_num_vars] = X_housing_scaler.transform(X_test[X_housing_num_vars])

# To check all the variables
X_train.head()

# To describe the variables
X_train.describe()

# To check all the variables
X_test.head()

# To check the shape of the Train data
X_train.shape

# To check the shape of the Test data
X_test.shape

# To see head
y_train.head()

# To check null values
X_train.isnull().sum()

# To check null values
X_test.isnull().sum()

"""#### Removal of Highly Coorelated variables from Train data"""

# To recheck the correlation coefficients to know which variables are highly correlated
house_cor = X_train.corr()

# To plot the Heatmap for all the varaibles
fig = plt.figure(figsize = (35, 30))
sns.heatmap(house_cor, cmap="RdYlGn", annot = True )
plt.title("Correlation between various numercial varibales\n", fontdict={'fontsize':30,'fontweight':5,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':25,'fontweight':8,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':25,'fontweight':8,'color':'Brown'})
plt.show()

# To collect and see absolute coorelation matrix
house_matrix = X_train.corr().abs()
house_matrix

# To find out Upper Trainangle
upper_tri_2 = house_matrix.where(np.triu(np.ones(house_matrix.shape), k=1).astype(np.bool))
upper_tri_2

# To find out the features which are highly correlated
to_drop_2 = [column for column in upper_tri_2.columns if any(upper_tri_2[column] > 0.80)]
to_drop_2

"""###### As per the analysis the above variables/features are highly correlated with each other therefore we need to drop all of them.

#### Dropping highly correlated variables.
"""

X_train = X_train.drop(['MSZoning_RM','Neighborhood_Somerst','BldgType_2fmCon','BldgType_Duplex','BldgType_TwnhsE',
                      'HouseStyle_1.5Unf','HouseStyle_SLvl','RoofStyle_Hip','RoofStyle_Shed','Exterior2nd_Brk Cmn',
                      'Exterior2nd_CBlock','Exterior2nd_CmentBd','Exterior2nd_HdBoard','Exterior2nd_MetalSd',
                      'Exterior2nd_VinylSd','Exterior2nd_Wd Sdng','MasVnrType_None','ExterQual_TA','ExterCond_TA',
                      'BsmtQual_TA','Electrical_Mix','KitchenQual_TA','GarageFinish_NA','GarageQual_NA','GarageCond_NA',
                      'SaleCondition_Partial'], 1)

X_test = X_test.drop(['MSZoning_RM','Neighborhood_Somerst','BldgType_2fmCon','BldgType_Duplex','BldgType_TwnhsE',
                      'HouseStyle_1.5Unf','HouseStyle_SLvl','RoofStyle_Hip','RoofStyle_Shed','Exterior2nd_Brk Cmn',
                      'Exterior2nd_CBlock','Exterior2nd_CmentBd','Exterior2nd_HdBoard','Exterior2nd_MetalSd',
                      'Exterior2nd_VinylSd','Exterior2nd_Wd Sdng','MasVnrType_None','ExterQual_TA','ExterCond_TA',
                      'BsmtQual_TA','Electrical_Mix','KitchenQual_TA','GarageFinish_NA','GarageQual_NA','GarageCond_NA',
                      'SaleCondition_Partial'], 1)

# To recheck the correlation coefficients to know which variables are highly correlated
fig = plt.figure(figsize = (35, 30))
house_cor_1 = X_train.corr()
sns.heatmap(house_cor_1, cmap="RdYlGn", annot = True )
plt.title("Correlation between various numercial varibales\n", fontdict={'fontsize':30,'fontweight':5,'color':'Green'})
plt.xlabel("\nNumerical Variables",fontdict={'fontsize':25,'fontweight':8,'color':'Brown'})
plt.ylabel("\nNumerical Variables\n",fontdict={'fontsize':25,'fontweight':8,'color':'Brown'})
plt.show()

# To check the shape of the Train data
X_train.shape

# To check the shape of the Test data
X_test.shape

# To check the variables of the dataframe
X_train.head()

"""### Step 8. Model Building

This time, we will be using the **LinearRegression function from SciKit Learn** for its compatibility with RFE (which is a utility from sklearn)

##### Feature Selection Using RFE

### A) Do simple Linear Regression
"""

# Running RFE with the output number of the variable equal to 30
lm_house = LinearRegression()
lm_house.fit(X_train, y_train)

# To Predict Train data
y_train_pred = lm_house.predict(X_train)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))

# To Predict Test data
y_test_pred = lm_house.predict(X_test)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))

# To check model parameters for Simple Linear Regression
lr_param = list(lm_house.coef_)
lr_param.insert(0, lm_house.intercept_)
lr_param = [round(x, 3) for x in lr_param]
lr_cols = X_train.columns
lr_cols = lr_cols.insert(0, "constant")
sorted(list(zip(lr_param, lr_cols)), key=lambda x:abs(x[0]), reverse=True)

"""#### To do RFE"""

# Running RFE
house_rfe = RFE(lm_house, 30)
house_rfe = house_rfe.fit(X_train, y_train)

# To check the list of RFE
list(zip(X_train.columns, house_rfe.support_, house_rfe.ranking_))

# To collect the columns supported by RFE
col_house = X_train.columns[house_rfe.support_]
col_house

"""#### Model 1."""

# Creating X_train dataframe with RFE selected variables
X_train_rfe = X_train[col_house]

# Adding a constant variable
X_train_rfe_house = sm.add_constant(X_train_rfe)

# To check the shape
X_train_rfe.shape

# Running the linear model
lm_house = sm.OLS(y_train, X_train_rfe_house).fit()
print(lm_house.summary())

"""### Checking VIF

Variance inflation Factor or VIF, gives a basic quantitative idea how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating VIF is:

### $ VIF_i = \frac{1}{1 - {R_i}^2} $

### We could have:
- High P-value, High VIF---Remove these variables
- High-low:
     - High P, low VIF--- Remove these variables first
     - Low P, high VIF--- Remove these after the one above
     - Low P, Low VIF--- Keep these variables
     
     
 - VIF Values
      - VIF < 5 is Good --- no need to eliminate this variable
      - VIF > 5 can be Okay --- need to inspect
      - VIF > 10 is definitely high and the respective variable need to remove

 - P value < 0.05 is Good.
"""

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 2."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["MSSubClass_75"], axis = 1)

# Adding a constant variable
X_train_lm_MSSub75 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_MSSub75 = sm.OLS(y_train, X_train_lm_MSSub75).fit()

# To take a print of subclass
print(lm_house_MSSub75.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 3."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["Exterior2nd_BrkFace"], axis = 1)

# Adding a constant variable
X_train_lm_3 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_3 = sm.OLS(y_train, X_train_lm_3).fit()

# To take a print of subclass
print(lm_house_3.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 4."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["RoofMatl_Metal"], axis = 1)

# Adding a constant variable
X_train_lm_4 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_4 = sm.OLS(y_train, X_train_lm_4).fit()

# To take a print of subclass
print(lm_house_4.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 5."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["LandSlope_Sev"], axis = 1)

# Adding a constant variable
X_train_lm_5 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_5 = sm.OLS(y_train, X_train_lm_5).fit()

# To take a print of subclass
print(lm_house_5.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 6."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["SaleCondition_Alloca"], axis = 1)

# Adding a constant variable
X_train_lm_6 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_6 = sm.OLS(y_train, X_train_lm_6).fit()

# To take a print of subclass
print(lm_house_6.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""### Model 7."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["Neighborhood_Veenker"], axis = 1)

# Adding a constant variable
X_train_lm_7 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_7 = sm.OLS(y_train, X_train_lm_7).fit()

# To take a print of subclass
print(lm_house_7.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Model 8."""

# Dropping a variable which has higher P and/or higher VIF value
X_train_rfe = X_train_rfe.drop(["Exterior1st_BrkFace"], axis = 1)

# Adding a constant variable
X_train_lm_8 = sm.add_constant(X_train_rfe)

# Running the linear model
lm_house_8 = sm.OLS(y_train, X_train_lm_8).fit()

# To take a print of subclass
print(lm_house_8.summary())

# Create a dataframe that will contain the names of all the feature variables and the respect.
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""#### Making Predictions with Final Model 8"""

# To check final column names
X_train_lm_8.columns

"""#### Inferences:
- Linear Regression is very lengthy and time consuming process.
- As per the above analysis we got our final model after 8 lengthy steps.
- In the final model (model - 8) all the featured variables has good P, and VIF values.
- Got 23 variables out of 30 auto selected variables.
- Whereas R-Squared value is $83.7$ %.

### B) Ridge Regression
"""

# list of alphas to tune
params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1,
 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0,
 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}

ridge = Ridge()

# cross validation
folds = 5
house_cv = GridSearchCV(estimator = ridge,
                        param_grid = params,
                        scoring= 'r2',
                        cv = folds,
                        return_train_score=True,
                        verbose = 1)
house_cv.fit(X_train, y_train)

# To see resultant output of Alpha values
house_cv_reslt = pd.DataFrame(house_cv.cv_results_)
house_cv_reslt.head()

# To plot mean test and train scores with alpha values
house_cv_reslt['param_alpha'] = house_cv_reslt['param_alpha'].astype('int32')

# To plot the graph
plt.figure(figsize=(15,6))
plt.plot(house_cv_reslt['param_alpha'], house_cv_reslt['mean_train_score'])
plt.plot(house_cv_reslt['param_alpha'], house_cv_reslt['mean_test_score'])
plt.xlabel("\nAlpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("R-Squared Value\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.title("R-Squared and Alpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.legend(['Train Score', 'Test Score'], loc='upper right')
plt.xlim(0, 100)
plt.xticks(np.arange(0, 100, 5))
plt.grid(color='g', linestyle='--', linewidth=1)
plt.show()

# To get the Best Estimator, Best Score, and Best Parameters
print("\n Best Score among aLL searched parameters:\n",
          house_cv.best_score_)

print("\n Best Estimator among aLL searched parameters:\n",
          house_cv.best_estimator_)

# To check with the best hyper paramter value
alpha = 10
ridge = Ridge(alpha=alpha)
ridge.fit(X_train, y_train)

# To see all the Ridge Coeffecients
ridge.coef_

# To Predict Train and Test data
y_train_pred = ridge.predict(X_train)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))

y_test_pred_ridge = ridge.predict(X_test)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred_ridge))

"""#### Inferences
- For Train and Test data Prediction values for Ridge Regression are $92.09$% and $88.7$% respectively.

"""

# To check Ridge Model parameters
ridge_param = list(ridge.coef_)
ridge_param.insert(0, ridge.intercept_)
ridge_param = [round(x, 3) for x in ridge_param]
us_cols = X_train.columns
us_cols = us_cols.insert(0, "constant")
sorted(list(zip(ridge_param, us_cols)), key=lambda x:abs(x[0]), reverse=True)

# To get top 10 features from Ridge model which are affecting target variable (Sales Price)
for k,l in sorted(list(zip(ridge_param, us_cols)), key=lambda x:abs(x[0]), reverse=True)[1:11]:
    print(f'{l:40}Score: {k}')

"""#### Inferences:
- As per the above Ridge Regression analysis we got final list of all the featured variables.
- From the Ridge Regression at Alpha = 10 we got following top 5 variables.
  1. 'GrLivArea'- 16424.82,
  2. 'MSZoning_FV' - 14425.868,
  3. 'YearBuilt' - 12298.454,  
  4. 'Neighborhood_Crawfor' - 12108.968,
  5. 'Functional_Typ' - 11642.468

### Double the Hyper Parameter Value
"""

# To check with the best hyper paramter value
alpha_2 = 20
ridge_2 = Ridge(alpha=alpha_2)
ridge_2.fit(X_train, y_train)
ridge_2.coef_

# To Predict Train and Test data
y_train_pred_2 = ridge_2.predict(X_train)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred_2))

y_test_pred_ridge_2 = ridge_2.predict(X_test)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred_ridge_2))

"""#### Inferences
- After making double the value of Alpha for Ridge regression value of R2-Score for Train and Test data is $91.37$% and $88.52$% respectively.
- Whereas previously it was $91.37$%, $88.52$% respectively.

"""

# To check Ridge Model parameters
ridge_2_param = list(ridge_2.coef_)
ridge_2_param.insert(0, ridge_2.intercept_)
ridge_2_param = [round(x, 3) for x in ridge_2_param]
us_cols_2 = X_train.columns
us_cols_2 = us_cols_2.insert(0, "constant")
sorted(list(zip(ridge_2_param, us_cols_2)), key=lambda x:abs(x[0]), reverse=True)

# To get top 10 features from Ridge model which are affecting target variable (Sales Price)
for k,l in sorted(list(zip(ridge_2_param, us_cols_2)), key=lambda x:abs(x[0]), reverse=True)[1:11]:
    print(f'{l:40}Score: {k}')

"""#### Inferences:
- After making double the value of Hyper Parameter for Ridge Regression we got the final list of all the featured variables.

- From the Ridge Regression at Alpha = 20 we got following top 5 variables.

  1. 'GrLivArea'- 15276.771,
  2. 'MSZoning_FV' - 10839.48,
  3. 'OverallQual' - 10714.089,
  4. 'YearBuilt' - 10703.717,
  5. 'TotalBsmtSF' - 9682.547.

### C) Lasso Regression
"""

# list of alphas to tune
L_params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1,
 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0,
 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}

# To implement Lasso Regression
lasso = Lasso()

# For Cross Validation
folds_L = 5
house_Lcv = GridSearchCV(estimator = lasso,
                        param_grid = L_params,
                        scoring= 'r2',
                        cv = folds_L,
                        return_train_score=True,
                        verbose = 1)
house_Lcv.fit(X_train, y_train)

# To see resultant output
house_Lcv_reslt = pd.DataFrame(house_Lcv.cv_results_)
house_Lcv_reslt.head()

# To plot mean test and train scores with alpha values
house_Lcv_reslt['param_alpha'] = house_Lcv_reslt['param_alpha'].astype('float32')

# To plot the graph
plt.figure(figsize=(15,6))
plt.plot(house_Lcv_reslt['param_alpha'], house_Lcv_reslt['mean_train_score'])
plt.plot(house_Lcv_reslt['param_alpha'], house_Lcv_reslt['mean_test_score'])
plt.xlabel("\nAlpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("R2-Square\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.title("R2-Square and Alpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.legend(['Train Score', 'Test Score'], loc='upper right')
plt.xlim(0, 200)
plt.xticks(np.arange(0, 200, 10))
plt.grid(color='g', linestyle='--', linewidth=1)
plt.show()

# To get the Best Estimator, Best Score, and Best Parameters
print("\n Best Estimator among aLL searched parameters:\n",
          house_Lcv.best_estimator_)
print("\n Best Score among aLL searched parameters:\n",
          house_Lcv.best_score_)

# To check with the best hyper paramter value
alpha = 100
lasso = Lasso(alpha=alpha)
lasso.fit(X_train, y_train)

# To check all the Lasso Coefficient
lasso.coef_

# To Predict Train ad Test data
y_train_pred = lasso.predict(X_train)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))

y_test_pred_lasso = lasso.predict(X_test)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred_lasso))

"""#### Inferences
- For Lasso Regression Train and Test data Prediction values are $91.63$% and $88.76$% respectively when Alpha is 100.

"""

# To check Lasso Model parameters
lasso_param = list(lasso.coef_)
lasso_param.insert(0, lasso.intercept_)
lasso_param = [round(x, 3) for x in lasso_param_2]
las_cols = X_train.columns
las_cols = las_cols.insert(0, "constant")
lasso_final_features = []
for k,l in sorted(list(zip(lasso_param, las_cols)), key=lambda x:abs(x[0]), reverse=True)[1:31]:
    lasso_final_features.append(l)
Laso_list = sorted(list(zip(lasso_param, las_cols)), key=lambda x:abs(x[0]), reverse=True)
Laso_list

# To check the length of the list
len(Laso_list)

# To get top 10 features from Ridge model which are affecting target variable (Sales Price)
for k,l in sorted(list(zip(lasso_param, las_cols)), key=lambda x:abs(x[0]), reverse=True)[1:11]:
    print(f'{l:40}Score: {k}')

"""#### Inferences:
- From the Lasso Regression at Alpha = 100 we got following top 5 variables.
  1. 'GrLivArea'- 18522.457,
  2. 'MSZoning_FV' - 16726.251,
  3. 'Neighborhood_Crawfor' - 14582.784,
  4. 'YearBuilt' - 11716.072,
  5. 'OverallQual' - 11290.349.

#### Double the Alpha for Lassoo
"""

# To check with the best hyper paramter value
alpha_2 = 200
lasso_2 = Lasso(alpha=alpha_2)
lasso_2.fit(X_train, y_train)
lasso_2.coef_

# To Predict Train and Test data
y_train_pred_las_2 = lasso_2.predict(X_train)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred_las_2))

y_test_pred_lasso_2 = lasso_2.predict(X_test)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred_lasso_2))

"""#### Inferences
- After making Alpha double from previous value that is 200, Train and Test data Prediction values for Lasso Regression are $90.28$% and $88.04$% respectively.

"""

# To check the length
len(Laso_list_2)

# To check Lasso Model parameters
lasso_param_2 = list(lasso_2.coef_)
lasso_param_2.insert(0, lasso_2.intercept_)
lasso_param_2 = [round(x, 3) for x in lasso_param_2]
las_cols_2 = X_train.columns
las_cols_2 = las_cols_2.insert(0, "constant")
lasso_final_features_2 = []
for k,l in sorted(list(zip(lasso_param_2, las_cols_2)), key=lambda x:abs(x[0]), reverse=True)[1:31]:
    lasso_final_features_2.append(l)
Laso_list_2 = sorted(list(zip(lasso_param_2, las_cols_2)), key=lambda x:abs(x[0]), reverse=True)
Laso_list_2

# To get top 10 features from Ridge model which are affecting target variable (Sales Price)
for k,l in sorted(list(zip(lasso_param_2, las_cols_2)), key=lambda x:abs(x[0]), reverse=True)[1:11]:
    print(f'{l:40}Score: {k}')

"""#### Inferences:
- After making double the value of Hyper Parameter for Lasso Regression we got the final list of all the featured variables.


- From the Lasso Regression at Alpha = 200 we got following top 5 variables.


  1. 'GrLivArea' - 18522.457,
  2. 'MSZoning_FV' - 16726.251,
  3. 'Neighborhood_Crawfor' - 14582.784,
  4. 'YearBuilt' - 11716.072,
  5. 'OverallQual' - 11290.349

### After removing top 5 contributors from Lasso at Alpha = 100
"""

# To drop top 5 variables from the Lasso model
X_train_T5 = X_train.drop(['GrLivArea', 'MSZoning_FV', 'Neighborhood_Crawfor', 'YearBuilt', 'OverallQual'], axis=1)

# To drop top 5 variables from the Lasso model
X_test_T5 = X_test.drop(['GrLivArea', 'MSZoning_FV', 'Neighborhood_Crawfor', 'YearBuilt', 'OverallQual'], axis=1)

# To cehck the shape
X_train_T5.shape

# To cehck the shape
y_train.shape

# To cehck the shape
X_test_T5.shape

# To fit the new train data
house_Lcv.fit(X_train_T5, y_train)

# To see resultant output of Alpha values
house_Lcv_reslt_5 = pd.DataFrame(house_Lcv.cv_results_)
house_Lcv_reslt_5.head()

# To plot mean test and train scores with alpha values
house_Lcv_reslt_5['param_alpha'] = house_Lcv_reslt_5['param_alpha'].astype('float32')

# To plot the graph
plt.figure(figsize=(15,6))
plt.plot(house_Lcv_reslt_5['param_alpha'], house_Lcv_reslt_5['mean_train_score'])
plt.plot(house_Lcv_reslt_5['param_alpha'], house_Lcv_reslt_5['mean_test_score'])
plt.xlabel("\nAlpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.ylabel("R2-Square\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.title("R2-Square and Alpha\n",fontdict={'fontsize':15,'fontweight':5,'color':'Brown'})
plt.legend(['Train Score', 'Test Score'], loc='upper right')
plt.xlim(0, 200)
plt.xticks(np.arange(0, 200, 10))
plt.grid(color='g', linestyle='--', linewidth=1)
plt.show()

# To get the Best Estimator, Best Score, and Best Parameters
print("\n Best Estimator among aLL searched parameters:\n",
          house_Lcv.best_estimator_)
print("\n Best Score among aLL searched parameters:\n",
          house_Lcv.best_score_)

# To check with the best hyper paramter value
alpha_5 = 100
lasso_5 = Lasso(alpha = alpha_5)
lasso_5.fit(X_train_T5, y_train)

# To check all the Lasso co-efficients
lasso_5.coef_

# To Predict Train ad Test data
y_train_pred_5 = lasso_5.predict(X_train_T5)
print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred_5))

y_test_pred_lasso_5 = lasso_5.predict(X_test_T5)
print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred_lasso_5))

"""#### Inferences
- After removal of top 5 variables/features from previous model, Train and Test data Prediction values for Lasso Regression are $88.61$% and $84.52$% respectively.

"""

# To check Lasso Model parameters
lasso_param_5 = list(lasso_5.coef_)
lasso_param_5.insert(0, lasso_5.intercept_)
lasso_param_5 = [round(x, 3) for x in lasso_param_5]
lasso_cols_5 = X_train_T5.columns
lasso_cols_5 = lasso_cols_5.insert(0, "constant")
lasso_final_features_5 = []
for k,l in sorted(list(zip(lasso_param_5, lasso_cols_5)), key=lambda x:abs(x[0]), reverse=True)[1:31]:
    lasso_final_features_5.append(l)
Lasso_list_5 = sorted(list(zip(lasso_param_5, lasso_cols_5)), key=lambda x:abs(x[0]), reverse=True)
Lasso_list_5

# To check the length of final parameters
len(Lasso_list_5)

# To get top 10 features from Ridge model which are affecting target variable (Sales Price)
for k,l in sorted(list(zip(lasso_param_5, lasso_cols_5)), key=lambda x:abs(x[0]), reverse=True)[1:11]:
    print(f'{l:40}Score: {k}')

"""#### Inferences:
- After removal of top 5 variables/features from the Lasso model then we got the final list of new top 5 featured variables.

- From the Lasso Regression at Alpha = 100 we got new following top 5 variables.

  1. 'TotalBsmtSF'  - 19508.675,
  2. '2ndFlrSF' - 17372.919,
  3. 'Neighborhood_OldTown' - (-) 15167.369,
  4. 'Neighborhood_Edwards' - (-) 14519.661,
  5. 'Neighborhood_Gilbert' - (-) 12997.745

### Conclusion

- The **Linear Regression** model is very lengthy, time consuming and complex model to find out the top features of the model. The R-Square value for this model is $83.7$%.

- The optimum value of Alpha for **Ridge Regression** is 10, whereas the R-Squared value for Train and Test data is $92.09$% and $88.71$% respectively.
- Top 5 features for this value are
                                     ('GrLivArea'- 16424.82,
                                     'MSZoning_FV' - 14425.868,
                                     'YearBuilt' - 12298.454,
                                     'Neighborhood_Crawfor' - 12108.968,
                                     'Functional_Typ' - 11642.468)
                                     
- After **double the Alpha in Ridge Regression**, the R-Squared value for Train and Test data is $91.37$% and $88.52$%, that is reduced a bit in comparsion to previous value.
- Whereas there is a slite change in top 5 features and their respective score value.
                                     ('GrLivArea'- 15276.771,
                                      'MSZoning_FV' - 10839.48,
                                      'OverallQual' - 10714.089,
                                      'YearBuilt' - 10703.717,
                                      'TotalBsmtSF' - 9682.547)
                                      
- The optimum value of Alpha for **Lasso Regression** is 100, whereas the R-Squared value for Train and Test data is $91.64$% and $88.76$% respectively.
- Top 5 features for this value are
                                    ('GrLivArea'- 18522.457,
                                     'MSZoning_FV' - 16726.251,
                                     'Neighborhood_Crawfor' - 14582.784,
                                     'YearBuilt' - 11716.072,
                                     'OverallQual' - 11290.349)

- After making **double the value of Alpha for Lasso Regression** the R-Squared value for Train and Test data is $90.28$% and $88.04$% respectively.
- Top 5 features for this value are
                                     ('GrLivArea' - 18522.457,
                                      'MSZoning_FV' - 16726.251,
                                      'Neighborhood_Crawfor' - 14582.784,
                                      'YearBuilt' - 11716.072,
                                      'OverallQual' - 11290.349)

- The change in double the value of Alpha for Lasso makes changes in R-Squared value of Train and Test data.

- After **removal of top 5 features selected by Lasso** there is a change in top 5 features. R-Squared value for Train and Test data is $88.61$% and $84.52$% respectively.
- Now there are new other top 5 features.
- Top 5 features for this value are
                                    ('TotalBsmtSF'  - 19508.675,
                                     '2ndFlrSF' - 17372.919,
                                     'Neighborhood_OldTown' - (-) 15167.369,
                                     'Neighborhood_Edwards' - (-) 14519.661,
                                     'Neighborhood_Gilbert' - (-) 12997.745}
                                     
- From all the above model it is concluded that Linear Regression Model is complex, lengthy and time consuming.
- Whereas among all the three Regression models (Linear, Ridge, Lasso) Ridge model is faster, robust and simple.
- R-Squared value for Train and Test data is $92.09$% and $88.71$% which is highest among all the models,therefore we should go for Ridge Regression model for our purpose.

### ***End of Advance Regression US-Housing***
"""

